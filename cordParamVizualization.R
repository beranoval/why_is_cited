# This script is an extended version of cordParam.R, which is used to process
# only the Bow_PubTator_Conceptnet_rule matrix to generate rule visualizations
# in the article (Table 11, Figure 5, Figure 6)

#inputs
library(enc)

rm(list = ls())
result_path <- "./results/cba_on_extra_cleaned/"
input_path <- "./results/matrices/"
inputmatrix_name <- "Bow_PubTator_Conceptnet"
exp_name <- "Bow_PubTator_Conceptnet_dedupl" #also used for output matrix name
metadataFileCSVfile <- "./inputs/metadata_with_opencitation.csv"

matrixNameXtrain <- paste0(input_path,"X_train_",inputmatrix_name,"_rule.csv")
matrixNameXtest <-  paste0(input_path,"X_test_",inputmatrix_name,"_rule.csv")
matrixNameytrain <-  paste0(input_path,"y_train_",inputmatrix_name,"_rule.csv")
matrixNameytest <-  paste0(input_path,"y_test_",inputmatrix_name,"_rule.csv")

#outputs
matrixNameXtrain_dedupl <-paste0(input_path,"X_train_",exp_name,"_rule.csv") #preprocessed matrix (cleaning applied)
matrixNameXtest_dedupl <-paste0(input_path,"X_test_",exp_name,"_rule.csv") #preprocessed matrix (cleaning applied)
coveredInstancesFile<-paste0(result_path,exp_name,"-matches.csv")
selectedRulesGraphFile<- paste0(result_path,exp_name,"selected-rules-graph.pdf") #Figure 6, note that the clustering has a random element, therefore each clustering is slightly different
classificationFile <- paste0(result_path,exp_name,"-classifications.csv") #which rule in the CBA classifier covered which instance
groupedPlotFile<-paste0(result_path,exp_name,"grouped-plot.pdf") #Figure 5, note that the clustering has a random element,  therefore each clustering is slightly different

candRulesFile <- paste0(result_path,exp_name,"-cand_rules.csv") # list of rules generated by association rule learning

rulesFile <-paste0(result_path,exp_name,"-CBA-rules.csv") # list of discovered rules with CBA
predictionFile<-paste0(result_path,exp_name,"-rulesPrediction.csv")

result_file_stats_cand<-paste0(result_path,exp_name,"-cand_rules_stats.csv") #statistics in Table 11
result_file_interesting_cand <-paste0(result_path,exp_name,"-cand_rules_interesting-rules.csv")

result_file_stats_cba<-paste0(result_path,exp_name,"-cba_stats.csv")
result_file_interesting_cba <-paste0(result_path,exp_name,"-cba_rules.csv")

rulesForSpecificConceptsInCandidateRules<-paste0(result_path,exp_name,"-","{concept}","-cand_rules") #this will be saved as csv and html
rulesForSpecificConceptsInCBARules<-paste0(result_path,exp_name,"-","{concept}","-cba_rules") #this will be saved as csv and html

figfilename <- paste(result_path,exp_name, ".png", sep="")

library(arc) #1.3.2
library(ROCR) #1.0-7
library(arules) #1.6-6
library(arulesViz) #1.3-3
library(stringr)
library(SnowballC) #0.7.0
library(stopwords) #2.2
library(textstem) #0.1.4
library(igraph) # version 1.2.6
library(rCBA) # version 0.4.3
library(rJava) # version 0.9-11


mergeSimilar=TRUE
subsetRules <- function(concepts,ruleModel,result_file_stats,result_file_interesting,rulesForSpecificConceptsFile)  
{
  
    rules.all.interesting <- subset(ruleModel, subset = lhs %pin% "xyz12345")
  write(c("concept","matchesLHSandHIGH","matchesLHSandLOW"), ncolumns =3, file = result_file_stats,append=FALSE)
  
  for (concept in concepts)
  {
    message(concept)
    #first check if item exists, otherwise we would get an error
    matchesLHS <- sum(ruleModel@lhs@itemInfo == concept)
    
    if (matchesLHS>0)
    {
      rules.sub  <- subset(ruleModel, subset = lhs %in% concept) #%in% for partial match
    }
    else
    {
      rules.sub  <- subset(ruleModel, subset = lhs %pin% concept) #%in% for partial match
    }
    matchesLHSandHIGH <- length(subset(rules.sub, subset = rhs %in% "Target=high"))
    matchesLHSandLOW <- length(subset(rules.sub, subset = rhs %in% "Target=low"))
    
    message(paste0("Rules found ", length(rules.sub)))
    message(paste0("Predicting HIGH ", matchesLHSandHIGH))
    message(paste0("Predicting LOW ", matchesLHSandLOW))
    write(c(concept,matchesLHSandHIGH,matchesLHSandLOW), ncolumns=3, file = result_file_stats,append=TRUE)
    if (length(rules.sub) !=0)
    {
      result_detailed <-gsub("\\{concept\\}",concept,rulesForSpecificConceptsFile)
      htmlwidgets::saveWidget(inspectDT(rules.sub), paste0(result_detailed,".html"), selfcontained = TRUE)
      write(rules.sub, file = paste0(result_detailed,".csv"))
      rules.all.interesting <-union(rules.sub,rules.all.interesting)
    }
  }
  write(rules.all.interesting, file = result_file_interesting)
  return (rules.all.interesting)
}
mergeSimilarColumns<- function(df,tolower=FALSE,removeUnderscores=TRUE,lemmatize=FALSE, stem=FALSE, removestopwords=TRUE){
  message("all to lower case")
  #names(df) <- tolower(names(df))
  df_new <- df[,FALSE] #remove all columns
  stopwordsOmitted<-0
  for (i in names(df))
  {
    if (removestopwords)
    {
      if (i %in% stopwords("en", source = "snowball"))
      {
        stopwordsOmitted<-stopwordsOmitted+1
        next
      }
    }
    newname <-i
    if (tolower)  newname<-tolower(newname)
    if (removeUnderscores) newname<-str_replace(newname, "_.*", "")
    if (lemmatize)  newname <- lemmatize_words(newname, dictionary = lexicon::hash_lemmas)
    if (stem) newname <- wordStem(newname, language = "english")
    if (newname %in% names(df_new))
    {
      #message(paste0("Merging ", i, " into ", newname))
      df_new[,newname]<- as.integer(df_new[,newname] | df[,i])
    }
    else
    {
      paste0("Copying ", i, " into ", newname)
      df_new[,newname]<- df[,i]
    }
  }
  message(paste0("original columns ",ncol(df)))
  message(paste0("new columns ",ncol(df_new)))
  message(paste0("stopwordsOmitted ",stopwordsOmitted))
  return(df_new)
}
frameToRules <- function(model){
  # export quality measures
  quality<-model[,2:6]
  # parse text
  rowItems <- lapply(model$rules,function(x) {
    x <- as.character(x)
    pattern <- "[[:space:]]*\\{(.*)\\}[[:space:]]*=>[[:space:]]*\\{(.*)\\}[[:space:]]*"
    m <- regexec(pattern, x)
    strRule <- regmatches(x, m)
    ant <- strsplit(strRule[[1]][2],",")[[1]]
    cons <- strsplit(strRule[[1]][3],",")[[1]]
    list(ant=ant,cons=cons)
  })
  # unique lhs and rhs items
  antItems <- unique(unlist(sapply(rowItems, function(x) x$ant)))
  consItems <- unique(unlist(sapply(rowItems, function(x) x$cons)))
  # all items
  items <- c(antItems, consItems)
  # prepare matrices for antecedents(lhs) and consequents(rhs)
  antM <- matrix(0, ncol=length(items), nrow = nrow(model))
  dimnames(antM) <- list(NULL, items)
  consM <- matrix(0, ncol=length(items), nrow = nrow(model))
  dimnames(consM) <- list(NULL, items)
  # set presence of items in lhs and rhs
  sapply(seq_len(nrow(model)), function(x){
    row <- unname(rowItems[x])[[1]]
    antM[x,match(row$ant,items)] <<- 1
    consM[x,match(row$cons,items)] <<- 1
    NULL
  })
  # convert to item matrix
  antI <- as(antM, "itemMatrix")
  consI <- as(consM, "itemMatrix")
  # create rules
  rules <- new("rules", lhs=antI, rhs=consI, quality = quality)
  rules
}
getMatchesForRule <- function (rule,trans, dois,result_path,exp_name) {
  if (is.factor(rule))
  {
    message(rule)
    rule <- as.character(rule)
    model <- data.frame("rules" = c(rule),
                        "support" = c(0.5),
                        "confidence" = c(0.5),
                        "lift" = c(1.0))
    ruleModel <- rCBA::frameToRules(model)
  }
  else
  {
    inspect(rule)
    ruleModel<-rule
    rule<-as.character(as(rule, "data.frame")$rule)
  }
  tLHS <- unname(is.subset(ruleModel@lhs, trans))
  matchingInstancesLHS<-which(tLHS==TRUE)
  
  #get only correct predictions
  tRHS <- unname(is.subset(ruleModel@rhs, trans))
  matchingInstancesRHS<-which(tRHS==TRUE)
  if (length(matchingInstancesLHS)==0)
  {
    message("no matching instances found")
    return()
  }
  else(
    message(paste0("matchingInstancesLHS:",length(matchingInstancesLHS)))
  )
  matchingInstances = intersect(matchingInstancesLHS,matchingInstancesRHS)
  
  message(paste0("matchingInstancesLHS+RHS:",length(matchingInstances)))
  fps<-length(matchingInstancesLHS)-length(matchingInstances) >0
  if (fps)
  {
    message(paste0("This rule has some false positives",fps))
    message(setdiff(matchingInstancesLHS,matchingInstancesRHS))
  }
  #   df <- data.frame(rule=rule,doi=dois[matchingInstancesLHS,]$Row.names, title=dois[matchingInstancesLHS,"title"], correct=matchingInstancesLHS %in% matchingInstancesRHS)
lhsDOIs <- trans[matchingInstancesLHS]@itemsetInfo$transactionID
rhsDOIs <- trans[matchingInstancesRHS]@itemsetInfo$transactionID
df <- data.frame(rule = character(0), doi = character(0), title=character(0), correct=logical(0), stringsAsFactors=F)

for (lhsDOI in lhsDOIs)
{
  toAdd <- c(rule, lhsDOI, as.character(dois[dois$Row.names==lhsDOI,]$title), as.logical(lhsDOI %in% rhsDOIs) )
  toAdd
  df[nrow(df)+1,]<-toAdd
}
return (df)
}

message("Executing CBA")
result_path <-""
#these matrices are currently for the binary bow vectorization


if (!(exists("trainFold_X")))
{
  warning("Reading data from disk")
  trainFold_X <- read.csv(matrixNameXtrain,row.names = 1,check.names=FALSE)
  trainFold_Y <- read.csv(matrixNameytrain,row.names = 1)
  testFold_X <- read.csv(matrixNameXtest,row.names = 1,check.names=FALSE)
  testFold_Y <- read.csv(matrixNameytest,row.names = 1, stringsAsFactors=T)        
}

if (mergeSimilar)
{
  trainFold_X<-mergeSimilarColumns(trainFold_X)
  testFold_X<-mergeSimilarColumns(testFold_X)
  trainFold_X<-trainFold_X[,sort(names(trainFold_X))]
  testFold_X<-testFold_X[,sort(names(testFold_X))]
  write.csv(trainFold_X, file=matrixNameXtrain_dedupl)
  write.csv(testFold_X, file=matrixNameXtest_dedupl)
}

if (!(exists("candidate_rule_limit")))
{
  warning("Using 50000 as candidate_rule_limit")
  candidate_rule_limit <-50000
}

if (!(exists("exp_name")))
{
  warning("Using default exp_name to save to disk")
  exp_name <-"rules-cba.csv"
}

if (!(exists("result_path")))
{
  warning("Using default result path to save to disk")
  result_path <-"./results/rule_lists"
}

trainFold_X[] <- lapply(trainFold_X, as.logical)
testFold_X[] <- lapply(testFold_X, as.logical)
classAtt="Target"
classAtt_New <- "Target"  #this string has to be unique - not appear as token in data
Target<- as.factor(trainFold_Y[[classAtt]])
Target_oracle<- as.factor(testFold_Y[[classAtt]])
trainFold <-cbind(trainFold_X,Target)
colnames(trainFold) <- c(colnames(trainFold_X),classAtt_New)
trans <- as(trainFold, "transactions")
appearance <- arc::getAppearance(trainFold, classAtt_New)

rules <- apriori(trans, parameter =
                   list(confidence = 0.5, support= 0.01, minlen=1, maxlen=4, maxtime=5), appearance=appearance)
#inspect orig model
rules_df <- DATAFRAME(rules, separate = TRUE)
write.csv(rules_df, file = candRulesFile)

concepts <-c("camel","dromed","feline","dog","rat","mouse","bat","cow","bovine","squirrel")


rules.all.interesting<-subsetRules(concepts,rules,result_file_stats_cand,result_file_interesting_cand,rulesForSpecificConceptsInCandidateRules)

#end inspect orig model

#this can be used to take only a subset of rules, now use all
#candidate_rule_limit<- 50000


message("Rule mining finished")
message(paste0("Trimming the rule list to ",candidate_rule_limit))
if (length(rules) > candidate_rule_limit)
{
  subs_rules<-rules[0:candidate_rule_limit]  
} else {
  subs_rules <- rules
}
message("CBA started (this may take a while)")
#this takes about 1 minute
rmCBA <- cba_manual(trainFold,subs_rules, trans, cutp = list(), appearance$rhs, classAtt_New)
message("CBA finished")
prediction <- predict(rmCBA, testFold_X)
message("Prediction finished")

rules_length <- length(rmCBA@rules)
avgRuleLengthCBA <- sum(rmCBA@rules@lhs@data)/length(rmCBA@rules)


write(rmCBA@rules, file = rulesFile)
write(prediction, file =  predictionFile)
#write(rmCBA@rules, file = "rules-confidences.csv")
message("Results written to disk")

message("COMPUTING WHICH RULE COVERED WHICH TRAINING INSTANCE")
append<-FALSE

ruleIDs <- predict(rmCBA, trainFold_X,outputFiringRuleIDs=TRUE)
for (i in 1:nrow(trainFold_X))
{
  doc <-rownames(trainFold_X)[i]

  write(c(doc,as.character(as(rmCBA@rules[ruleIDs[i]], "data.frame")$rule)), file = classificationFile,append=append, ncol=2)
  append<-TRUE
}

  message("COMPUTING EVALUATION STATS")

#inspect(rmCBA@rules)
#rules_df <- DATAFRAME(rmCBA@rules, separate = TRUE)

#confusion matrix
#https://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html#perclass
confusion_matrix_df = as.matrix(table(Actual = Target_oracle, Predicted = prediction))
confusion_matrix_df

n = sum(confusion_matrix_df) # number of instances
nc = nrow(confusion_matrix_df) # number of classes
diag = diag(confusion_matrix_df) # number of correctly classified instances per class 
rowsums = apply(confusion_matrix_df, 1, sum) # number of instances per class
colsums = apply(confusion_matrix_df, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
macroPrecision = mean(precision)
macroRecall = mean(recall)
macroF1 = mean(f1)
acc = sum(diag)/n
acc
message(paste0("accuracy: ",acc))
pred_perf_df <- data.frame(acc,macroPrecision, macroRecall, macroF1)
#ROC curve
confidence_scores <- predict(rmCBA, testFold_X, outputConfidenceScores=TRUE,confScoreType="global")
#target_cba <- droplevels(factor(testFold_Y[[classAtt]],ordered = TRUE,levels=levels(testFold_Y[[classAtt]])))
target_cba <- droplevels(factor(testFold_Y[[classAtt]],ordered = TRUE,levels=levels(Target)))
pred_cba <- ROCR::prediction(confidence_scores, target_cba)

if (FALSE) {
    roc_cba <- ROCR::performance(pred_cba, "tpr", "fpr")
    ROCR::plot(roc_cba, lwd=2, colorize=TRUE)
    lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1)
    dev.copy(png,figfilename)
    dev.off()
    message(paste0("roc written to ",figfilename))
    auc <- ROCR::performance(pred_cba, "auc")
    auc <- unlist(auc@y.values)
    auc    
}
message("CLUSTERING DISCOVERED RULES")
message("Reading rules from disk")
dfRules <- read.csv(rulesFile,
                    header = TRUE,sep = " " ) #sep = " "
if("LHS" %in% colnames(dfRules)){
  dfRules$rules = paste(unlist(dfRules["LHS"]), "=>", unlist(dfRules["RHS"]))
  dfRules <- subset(dfRules, select = -c(LHS,RHS))
  
}
ruleModel <- frameToRules(dfRules)
htmlwidgets::saveWidget(inspectDT(ruleModel),  file = paste0(result_path,exp_name,"-CBA-rules.html"), selfcontained = FALSE)

pdf(groupedPlotFile) 
plot(ruleModel, method = "grouped matrix",, lhs_items=4, k=30)
dev.off()

message("CREATE GRAPH OF SELECTED DISCOVERED RULES")

#this will create empty set of rules to which we will add
concepts <-c("camel","dromed","feline","dog","rat","mouse","bat","cow","bovine","squirrel")

rules.all.interesting<-subsetRules(concepts,ruleModel,result_file_stats_cba,result_file_interesting_cba,rulesForSpecificConceptsInCBARules)

pdf(selectedRulesGraphFile) 
plot(rules.all.interesting, method="graph",control = list(verbose = TRUE), 
         cex=0.8, 
      layout=igraph::with_dh(),
     )
dev.off()

concepts <-c(concepts,"dpp4")
rules.all.interesting<-subsetRules(concepts,ruleModel,result_file_stats_cba,result_file_interesting_cba,rulesForSpecificConceptsInCBARules)


message("Explaining rules")

metadataFileCSV <- read.csv(metadataFileCSVfile,
                            header = TRUE,row.names = "doi" ) #sep = " "
dois_titles <- merge(x = trainFold_Y, y = metadataFileCSV[,c("title"),drop=FALSE], by.x = 0, by.y = 0,all.x = TRUE)

append=FALSE 
for( i in 1:length(rules.all.interesting)) {
  #dfRules[i, "rules"]
  message(i)
  df<-getMatchesForRule(rules.all.interesting[i], trans, dois_titles,result_path,exp_name)
  write.table(df, file = coveredInstancesFile,append=append,col.names = !append )
  append=TRUE 
}

