{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why was this cited? Explainable Machine Learning Applied to COVID-19 Research Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contains code for all models that were created for the purpose of getting results for the article: **Why was this cited? Explainable Machine Learning Applied to COVID-19 Research Articles**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected directory structure\n",
    "\n",
    "* cache/Entity_matrix <- cached entity extraction results, the folder must exist even if empty\n",
    "* cache/FeatureImportance <- feature importance results used to prune rule learning matrices and for Shapley plots\n",
    "* inputs/PUBMED <- article records\n",
    "* inputs/metadata_with_opencitations.csv <- abstracts and article metadata for dataset version 1 with citations\n",
    "* inputs/df_sw_tok_low_punc_lemm_v5.csv <- abstracts and article metadata for dataset version 2\n",
    "* inputs/biblio.json <- journal information\n",
    "* inputs/author_names_info.csv <- author names and their nationality (dataset version 1)\n",
    "* inputs/citationcounts_oci_revised.csv <- citations for dataset version 2\n",
    "* results/ <- results of processing\n",
    "* Gollam/ <- several internally used code routines\n",
    "* cordParam.R <- code for CBA models\n",
    "* cordParamVisualization.R <- code for visualizing CBA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs of this notebook - description \n",
    "\n",
    "## DATASET VERSION 1 \n",
    "### 1) metadata_with_opencitations.csv\n",
    "- This is the main input source of articles used in this analysis (CORD-19 corpus downloaded there:https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) after reduction by PUBMED (downloaded https://github.com/fhircat/CORD-19-on-FHIR/tree/master/datasets/Pubtator_RDF/CORD-19-Abstracts) with added extracted Opencitations. The whole process how to receive this data from data sources and how it is cleaned is described in notebook preprocessing_dataset_v1. \n",
    "\n",
    "### 2) biblio.json\n",
    "- This is input source for bibliometric features, used only with dataset version 1. \n",
    "\n",
    "### 3) author_names_info.csv\n",
    "- There is a list of names used in author_names features matrix after reduction for dataset version 1 with added information about nationality of the name received from https://www.name-prism.com/.\n",
    "\n",
    "\n",
    "## DATASET VERSION 2\n",
    "### 1) df_sw_tok_low_punc_lemm_v5.csv\n",
    "- This is newer version of CORD-19 corpus after cleaning of abstract and reduction - described in notebook preprocessing_dataset_v2.ipynb. \n",
    "\n",
    "### 2) citationcounts_oci_revised.csv\n",
    "- Extracted citations for dataset version 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS FOR DATASET VERSION 1 \n",
    "\n",
    "- This notebook is used for running results of article for 2 versions of datasets \n",
    "- Can be run for specific version of dataset by set the value of parameter DATASET_VERSION. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 1  # Other choise: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "import numpy\n",
    "import hashlib\n",
    "import itertools  \n",
    "import ast\n",
    "\n",
    "import sklearn.ensemble\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve,classification_report,confusion_matrix,mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "#BERT random forest\n",
    "import sys, setuptools, tokenize\n",
    "import torch\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# BERT neural network\n",
    "# need to make sure that you are running TensorFlow 2.0. Google Colab, by default, doesn't run your script on TensorFlow 2.0. \n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import math\n",
    "import random\n",
    "\n",
    "from Gollam.Data_Clean.CheckLanguage import isEnglish\n",
    "from Gollam.Ngrams.ExtractNgramsEntityList import extract_ngrams_entity_list\n",
    "from Gollam.Data_Clean.IsNotAscii import is_not_ascii\n",
    "\n",
    "import shap\n",
    "\n",
    "from rdflib import Graph\n",
    "import itertools\n",
    "from corels import CorelsClassifier\n",
    "\n",
    "## FOR CBA\n",
    "import en_core_sci_lg\n",
    "import en_ner_bionlp13cg_md\n",
    "import en_ner_jnlpba_md\n",
    "import en_ner_craft_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"R_HOME\"] = \"/usr/lib/R\" \n",
    "os.environ[\"R_HOME\"] = r\"C:\\Program Files\\R\\R-4.1.2\"\n",
    "#os.environ[\"PATH\"] = \"/usr/lib/R\"+  \";\" + os.environ[\"PATH\"] \n",
    "os.environ[\"PATH\"] = r\"C:\\Program Files\\R\\R-4.1.2\\bin\\x64\" + \";\" + os.environ[\"PATH\"]\n",
    "\n",
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri, packages\n",
    "pandas2ri.activate()\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.conversion import localconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from io import StringIO\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False - regenerate matrices, may be extremely slow\n",
    "#True - used cached matrices\n",
    "CSV_matrix_PubTator_conceptnet = True\n",
    "CSV_matrix_PubTator = True\n",
    "CSV_matrix_scispacy_conceptnet = True\n",
    "CSV_matrix_scispacy = True\n",
    "\n",
    "# Reduction of BOW and TF-IDF by parameter min_df\n",
    "MIN_DF = 32\n",
    "\n",
    "# Reduction of SCISPACY and PUBTATOR matrices by feature importance from random forest\n",
    "MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT = 1500\n",
    "\n",
    "# if  we use parameters from grid for random forest or default version (setted parameters are faster and results are same)\n",
    "USE_GRID = True\n",
    "\n",
    "RUN_SEARCHING_OPTIMAL_MATRIX_SIZE = False\n",
    "RUN_GRID_FOR_BERT = False\n",
    "RUN_GRID_RANDOM_FOREST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents from: https://allenai.org/data/cord-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    \n",
    "    DATA_PATH = \"./inputs/metadata_with_opencitation.csv\"\n",
    "    PubTator_DATA_PATH=\"./PUBMED/\"\n",
    "    documents = pd.read_csv(DATA_PATH, error_bad_lines=False,index_col=\"doi\").reset_index()\n",
    "    documents = documents.drop_duplicates(subset=['pubmed_id'], keep=False)\n",
    "    documents = documents.drop_duplicates(subset=['doi','journal'], keep=False)\n",
    "    documents = documents.set_index(\"doi\")\n",
    "\n",
    "    # impact factor features\n",
    "    with open('inputs/biblio.json') as f:\n",
    "         data = json.load(f)\n",
    "\n",
    "    #drop documents with short abstract\n",
    "    documents = documents.drop(documents[documents['abstract'].map(len) < 3].index)\n",
    "\n",
    "    # remove word abstract from the abstracts\n",
    "    documents[\"abstract\"] = documents[\"abstract\"].str.lower().replace('abstract','')\n",
    "    documents[\"abstract\"] = documents[\"abstract\"].str.replace(r'abstract', '')\n",
    "    documents['age']  = 2020- documents['year']\n",
    "    documents[\"NormCitations\"] = documents[\"OpenCitations\"]/documents['age']\n",
    "\n",
    "    DISCRETIZATION_NUMBER_OF_CATEGORIES=2\n",
    "    DISCRETIZATION_PRECISION=0\n",
    "    DISCRETIZATION_LABELS=[\"low\",\"high\"]\n",
    "    documents[\"Target\"]=pd.qcut(documents[\"NormCitations\"], q=DISCRETIZATION_NUMBER_OF_CATEGORIES, precision=DISCRETIZATION_PRECISION, labels=DISCRETIZATION_LABELS)\n",
    "    documents = documents.dropna()\n",
    "    print(len(documents))\n",
    "    \n",
    "if DATASET_VERSION == 2:\n",
    "    \n",
    "    df_all = pd.read_csv(\"inputs/df_sw_tok_low_punc_lemm_v5.csv\").rename(columns = {'doi_x':'doi'})\n",
    "    df_cit = pd.read_csv(\"inputs/citationcounts_oci_revised.csv\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "    df_cit = df_cit.rename(columns={'count_opencitations;;;;;;': 'OpenCitations'})\n",
    "    df_cit['OpenCitations'] =  df_cit['OpenCitations'].str.extract(r'(\\d+)', expand=False)\n",
    "    df_cit = df_cit[['doi','OpenCitations']].dropna()\n",
    "    df_cit['OpenCitations'] = df_cit['OpenCitations'].astype(int)\n",
    "\n",
    "    df_merged = df_cit.merge(df_all, on=\"doi\",how=\"inner\") # want to have articles from Kaggle with opencitations\n",
    "    df_merged = df_merged[pd.notnull(df_merged['Year'])] # for target - needed to have not null year\n",
    "    documents = df_merged\n",
    "    documents['year']=documents['Year']\n",
    "\n",
    "    #drop documents with short abstract\n",
    "    documents = documents.drop(documents[documents['abstract'].map(len) < 3].index)\n",
    "    \n",
    "    # remove word abstract from the abstracts\n",
    "    documents[\"abstract\"] = documents[\"abstract\"].str.lower().replace('abstract','')\n",
    "    documents[\"abstract\"] = documents[\"abstract\"].str.replace(r'abstract', '')\n",
    "    documents['age']  = 2021 - documents['year']\n",
    "    documents[\"NormCitations\"]= documents[\"OpenCitations\"]/documents['age']\n",
    "    DISCRETIZATION_NUMBER_OF_CATEGORIES=2\n",
    "    DISCRETIZATION_PRECISION=0\n",
    "    DISCRETIZATION_LABELS=[\"low\",\"high\"]\n",
    "    documents[\"Target\"]=pd.qcut(documents[\"NormCitations\"], q=DISCRETIZATION_NUMBER_OF_CATEGORIES,  precision=DISCRETIZATION_PRECISION,  labels=DISCRETIZATION_LABELS)\n",
    "    documents = documents.dropna()\n",
    "    documents[documents[\"Target\"]==\"low\"][[\"Target\",\"OpenCitations\"]].value_counts()\n",
    "    print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    # impact factor features\n",
    "    with open('inputs/biblio.json') as f:\n",
    "         data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    if CSV_matrix_PubTator_conceptnet == True:\n",
    "        matrix_PubTator_conceptnet_pd=pd.read_csv('cache/Entity_matrix/matrix_fhir_2940_conceptnet_new.csv',engine=\"python\",index_col=\"doi\", error_bad_lines=False)\n",
    "        print(matrix_PubTator_conceptnet_pd.shape)\n",
    "    \n",
    "    if CSV_matrix_PubTator == True:    \n",
    "        matrix_PubTator_pd=pd.read_csv('cache/Entity_matrix/matrix_fhir_2940.csv',engine=\"python\",index_col=\"doi\", error_bad_lines=False)\n",
    "        print(matrix_PubTator_pd.shape)\n",
    "    \n",
    "    if CSV_matrix_scispacy == True:\n",
    "        matrix_scispacy_pd=pd.read_csv('cache/Entity_matrix/matrix_scispacy_2940.csv',engine=\"python\",index_col=\"doi\", error_bad_lines=False)\n",
    "        print(matrix_scispacy_pd.shape)\n",
    "    \n",
    "    if CSV_matrix_scispacy_conceptnet == True:  \n",
    "        matrix_scispacy_conceptnet_pd=pd.read_csv('cache/Entity_matrix/matrix_scispacy_2940_conceptnet.csv',engine=\"python\",index_col=\"doi\", error_bad_lines=False)\n",
    "        print(matrix_scispacy_conceptnet_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = len(documents['year'].unique())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(documents['year'], num_bins)\n",
    "\n",
    "ax.set_xlabel('Year of publication')\n",
    "ax.set_ylabel('Articles')\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning this plot is done on data before documents for which we do not have biblio data have been removed\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#ax.scatter(documents[documents.Target==\"low\"][\"year\"], documents[documents.Target==\"low\"][\"Target\"])\n",
    "ax.hist(documents[documents.Target==\"low\"][\"year\"], num_bins, alpha=0.5)\n",
    "ax.hist(documents[documents.Target==\"high\"][\"year\"], num_bins, alpha=0.5, color=\"red\")\n",
    "\n",
    "plt.legend(['Low citations', 'High citations']) \n",
    "ax.set_xlabel('Year of publication')\n",
    "ax.set_ylabel('Number of articles')\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning this plot is done on data before documents for which we do not have biblio data have been removed\n",
    "documents[\"Target_OpenCitations\"]=pd.qcut(documents[\"OpenCitations\"],\n",
    "                            q=DISCRETIZATION_NUMBER_OF_CATEGORIES, \n",
    "                            precision=DISCRETIZATION_PRECISION, \n",
    "                            labels=DISCRETIZATION_LABELS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(documents[documents.Target_OpenCitations==\"low\"][\"year\"], num_bins, alpha=0.5)\n",
    "ax.hist(documents[documents.Target_OpenCitations==\"high\"][\"year\"], num_bins, alpha=0.5, color=\"red\")\n",
    "plt.legend(['Low citations', 'High citations']) \n",
    "ax.set_xlabel('Year of publication')\n",
    "ax.set_ylabel('Citations - low')\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "documents=documents.drop(columns=['Target_OpenCitations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    \n",
    "    def flatten_json(y):\n",
    "        out = {}\n",
    "\n",
    "        def flatten(x, name=''):\n",
    "            if type(x) is dict:\n",
    "                for a in x:\n",
    "                    flatten(x[a], name + a + '_')\n",
    "            elif type(x) is list:\n",
    "                i = 0\n",
    "                for a in x:\n",
    "                    flatten(a, name + str(i) + '_')\n",
    "                    i += 1\n",
    "            else:\n",
    "                out[name[:-1]] = x\n",
    "\n",
    "        flatten(y)\n",
    "        return out\n",
    "\n",
    "    dic_flattened = (flatten_json(item[1]) for item in data.items())\n",
    "    impacts_ais_jcats_df = pd.DataFrame(dic_flattened)\n",
    "    # use pubmedids as index\n",
    "    impacts_ais_jcats_df.index = list(data.keys())\n",
    "    #use only first wos and ford category \n",
    "    impacts_ais_jcats_df=impacts_ais_jcats_df[[ 'impact_0_impact', 'impact_1_impact', 'ais_0_ais','ais_1_ais', 'WoSkategorie_0_obor', \n",
    "                                           'WoSkategorie_0_aisQ', 'WoSkategorie_0_impactQ', 'FORD_0_ford', 'FORD_0_aisQ', 'FORD_0_impactQ']]\n",
    "    impacts_ais_jcats_df2=impacts_ais_jcats_df.reset_index().rename(columns={'index': 'pubmed_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    bibliometric_features=documents.reset_index()[[\"doi\",\"Target\",\"author_count\",\"journal\",\"pubmed_id\",\"license\"]]\n",
    "    bibliometric_features['pubmed_id']=bibliometric_features['pubmed_id'].astype(int)\n",
    "    bibliometric_features['pubmed_id']=bibliometric_features['pubmed_id'].astype(str)\n",
    "    bibliometric_features=bibliometric_features.merge(impacts_ais_jcats_df2,on='pubmed_id', how='left').dropna().set_index(\"doi\")\n",
    "    del bibliometric_features['pubmed_id']\n",
    "    \n",
    "    #target\n",
    "    target_bib_features = bibliometric_features.loc[:, bibliometric_features.columns == 'Target']\n",
    "    bibliometric_features = bibliometric_features.loc[:, bibliometric_features.columns != 'Target']\n",
    "    \n",
    "    # Reduce documents\n",
    "    new_ids =  bibliometric_features.reset_index()[[\"doi\"]].set_index(\"doi\")\n",
    "    documents = new_ids.merge(documents, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create several versions of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can be performed on complete dataset without risk of leakage, because the vectorization performed is binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(analyzer = \"word\", \n",
    "                       tokenizer = None, \n",
    "                       ngram_range=(1,3), \n",
    "                       lowercase = True,\n",
    "                       strip_accents = \"ascii\", \n",
    "                       binary= True,\n",
    "                       stop_words='english',\n",
    "                       min_df=MIN_DF) \n",
    "\n",
    "matrix_tfidf= vect_tfidf.fit_transform(documents['abstract'])\n",
    "tokens_tfidf = vect_tfidf.get_feature_names()\n",
    "matrix_tfidf_pd = pd.DataFrame.sparse.from_spmatrix(matrix_tfidf, columns = tokens_tfidf,index=documents.reset_index().doi)\n",
    "matrix_tfidf_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(analyzer = \"word\", \n",
    "                       tokenizer = None, \n",
    "                       ngram_range=(1,3), \n",
    "                       lowercase = True,\n",
    "                       strip_accents = \"ascii\", \n",
    "                       binary= True,\n",
    "                       stop_words='english',\n",
    "                       min_df=MIN_DF)    \n",
    "\n",
    "matrix_bow = cvec.fit_transform(documents['abstract'])\n",
    "tokens_bow = cvec.get_feature_names()\n",
    "matrix_bow_pd = pd.DataFrame.sparse.from_spmatrix(matrix_bow, columns = tokens_bow,index=documents.reset_index().doi)\n",
    "matrix_bow_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary document matrix with scispacy entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can be performed on complete dataset without risk of leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_matrix_scispacy == False:\n",
    " \n",
    "    #Scispacy imports\n",
    "    #pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz\n",
    "    #pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_ner_bionlp13cg_md-0.3.0.tar.gz\n",
    "    #pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_ner_jnlpba_md-0.3.0.tar.gz\n",
    "    #pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_ner_craft_md-0.3.0.tar.gz\n",
    "\n",
    "    nlp = en_core_sci_lg.load()\n",
    "    bionlp = en_ner_bionlp13cg_md.load()\n",
    "    jnlpba = en_ner_jnlpba_md.load()\n",
    "    craft = en_ner_craft_md.load()\n",
    "\n",
    "    matrix_scispacy_pd = pd.DataFrame(index = documents.index, columns=[])\n",
    "    for index, document in documents.iterrows():\n",
    "        print(\"processing \" + str(document.name))\n",
    "        abstract_lower = document['abstract']\n",
    "        ent_nlp = nlp(abstract_lower)\n",
    "        ent_bionlp = bionlp(abstract_lower)\n",
    "        ent_craft = craft(abstract_lower)\n",
    "        ent_jnlpba = jnlpba(abstract_lower)\n",
    "        all_ents=set(ent_nlp.ents + ent_bionlp.ents + ent_craft.ents + ent_jnlpba.ents)\n",
    "        #new - lemmatization\n",
    "        all_ents = map(lambda x: x.lemma_, all_ents)\n",
    "    \n",
    "        all_ents = filter(lambda x: not(is_not_ascii(str(x))), all_ents)\n",
    "        for ent in all_ents:\n",
    "            matrix_scispacy_pd.at[document.name,str(ent)]=1\n",
    "\n",
    "    matrix_scispacy_pd.shape  \n",
    "    matrix_scispacy_pd.to_csv('cache/Entity_matrix/matrix_scispacy_2940_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add ConceptNet entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_matrix_scispacy_conceptnet == False:  \n",
    "\n",
    "    def conceptNetExpansion(entitiesToExpand, debug=False):\n",
    "        entityDict = {}\n",
    "        i=0\n",
    "        for entity in entitiesToExpand:\n",
    "            expandedEntities = set()\n",
    "            if debug:\n",
    "                print(str(i) + \" processing \" + entity)\n",
    "            else:\n",
    "                print(\"e\" + filename, end='')\n",
    "            i=i+1\n",
    "            if i==5:\n",
    "                print(\"Max processed entities for TEST run reached\")\n",
    "                break\n",
    "            str_entity = entity.lower()\n",
    "            if len(str_entity.split(\" \"))>1:\n",
    "                entityList = extract_ngrams_entity_list(str_entity)  + [str_entity]\n",
    "            else:\n",
    "                entityList = [str_entity]    \n",
    "                \n",
    "            for internal_entity in entityList:\n",
    "                print(\"i\", end='')\n",
    "                print(\" processing internal entity: \" + internal_entity)\n",
    "                if type(internal_entity) == int:\n",
    "                    continue\n",
    "                hash_object = hashlib.md5(internal_entity.encode('utf-8'))\n",
    "                filename = hash_object.hexdigest()\n",
    "                try: \n",
    "                    with open('cache/ConceptNet/'+filename+\".json\", 'r') as cachefile:\n",
    "                        obj = json.load(cachefile)\n",
    "                        if debug:\n",
    "                            print(\"Read from cache \" + filename)\n",
    "                        else:\n",
    "                            print(\"c\" + filename, end='')\n",
    "                except IOError:\n",
    "                    url = 'http://api.conceptnet.io/c/en/'+internal_entity\n",
    "                    obj = requests.get(url).json()\n",
    "                    with open('cache/ConceptNet/'+filename+\".json\", 'w') as outfile:\n",
    "                        json.dump(obj, outfile)\n",
    "                        if debug:\n",
    "                            print(\"Saved to disk as \" + filename )\n",
    "                        else:\n",
    "                            print(\"s\" + filename, end='')\n",
    "                \n",
    "\n",
    "                for edge in obj['edges']:\n",
    "                    entityLinkedAsStart = str(edge['start']['label']) \n",
    "                    entityLinkedAsEnd = str(edge['end']['label'])# str(obj['edges'][i]['end']['label'])\n",
    "                    if isEnglish(entityLinkedAsStart):\n",
    "                        expandedEntities.add(entityLinkedAsStart)\n",
    "                    if isEnglish(entityLinkedAsEnd):\n",
    "                        expandedEntities.add(entityLinkedAsEnd)\n",
    "            entityDict[entity] = expandedEntities\n",
    "        return entityDict\n",
    "\n",
    "    #This no longer works reliably as conceptnet API takes too long to respond. Maybe download conceptnet and try locally, or let it run over night and cache the results? \n",
    "    def generateConceptNetExpansionMatrix(matrix, conceptNetExpansionDict):\n",
    "        matrix_expanded = pd.DataFrame(index = matrix.index, columns=[])\n",
    "        for inputEntity in conceptNetExpansionDict:\n",
    "            for conceptNetEntity in conceptNetExpansionDict[inputEntity]:\n",
    "                matrix_expanded.loc[matrix[inputEntity]==1,conceptNetEntity]=1\n",
    "        return(matrix_expanded)\n",
    "\n",
    "    scispacyToConceptNetExpansionDict=conceptNetExpansion(matrix_scispacy_pd.columns)\n",
    "    matrix_scispacy_conceptnet = generateConceptNetExpansionMatrix(matrix_scispacy_pd,scispacyToConceptNetExpansionDict)\n",
    "    matrix_scispacy_conceptnet.shape\n",
    "\n",
    "    matrix_scispacy_conceptnet.to_csv('cache/Entity_matrix/matrix_scispacy_2940_conceptnet.csv')\n",
    "    scispacyToConceptNetExpansionDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubTator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that \"full data\" was renamed to \"full_data\"\n",
    "PubTator processing uses all text nodes shorter than 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CSV_matrix_PubTator_conceptnet == False and CSV_matrix_PubTator == False:\n",
    "     \n",
    "    FHIR_DATA_PATH = 'inputs/PUBMED/'\n",
    "        \n",
    "    matrix_PubTator = pd.DataFrame(index = documents.index, columns=[])\n",
    "    entities_doc=set()\n",
    "\n",
    "    for index, document in documents.iterrows():\n",
    "        \n",
    "        path = FHIR_DATA_PATH+ str(int(document.pubmed_id)) + \".ttl\"\n",
    "        if os.path.isfile(path):\n",
    "            print(\"processing \" + path)\n",
    "            pubmed_record = open(path, 'r',encoding=\"utf8\") \n",
    "            g = Graph()\n",
    "            g.parse(pubmed_record, format='turtle')\n",
    "            qres = g.query(\n",
    "        \"\"\"SELECT DISTINCT ?text\n",
    "           WHERE {\n",
    "              ?a pm:text ?text .\n",
    "           }\"\"\")\n",
    "            for row in qres:\n",
    "                extractedEntity = str(row.asdict()['text'].toPython())\n",
    "                if len(extractedEntity) < 40 and not(is_not_ascii(str(extractedEntity))):                 \n",
    "                    matrix_PubTator.at[document.name,extractedEntity]=1\n",
    "\n",
    "\n",
    "    PubTatorToConceptNetExpansionDict=conceptNetExpansion(matrix_PubTator.columns)\n",
    "    matrix_PubTator_conceptnet = generateConceptNetExpansionMatrix(matrix_PubTator,PubTatorToConceptNetExpansionDict)\n",
    "    matrix_PubTator_conceptnet\n",
    "    \n",
    "    matrix_PubTator_conceptnet.to_csv('cache/Entity_matrix/matrix_fhir_2940_conceptnet_new_zkouska.csv') \n",
    "    matrix_PubTator.to_csv('cache/Entity_matrix/matrix_fhir_2940_zkouska.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    docSubsetWithMatchinbBiData = documents[documents.index.isin(target_bib_features.index)]\n",
    "if DATASET_VERSION == 2:\n",
    "    docSubsetWithMatchinbBiData = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(docSubsetWithMatchinbBiData[docSubsetWithMatchinbBiData.Target==\"low\"][\"year\"], num_bins, alpha=0.5)\n",
    "ax.hist(docSubsetWithMatchinbBiData[docSubsetWithMatchinbBiData.Target==\"high\"][\"year\"], num_bins, alpha=0.5, color=\"red\")\n",
    "\n",
    "plt.legend(['Low citations', 'High citations']) \n",
    "ax.set_xlabel('Year of publication')\n",
    "ax.set_ylabel('Number of articles')\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"results/HistAfterAgeNorm.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning this plot is done on data before documents for which we do not have biblio data have been removed\n",
    "docSubsetWithMatchinbBiData[\"Target_OpenCitations\"]=pd.qcut(docSubsetWithMatchinbBiData[\"OpenCitations\"],\n",
    "                            q=DISCRETIZATION_NUMBER_OF_CATEGORIES, \n",
    "                            precision=DISCRETIZATION_PRECISION, \n",
    "                            labels=DISCRETIZATION_LABELS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(docSubsetWithMatchinbBiData[docSubsetWithMatchinbBiData.Target_OpenCitations==\"low\"][\"year\"], num_bins, alpha=0.5)\n",
    "ax.hist(docSubsetWithMatchinbBiData[docSubsetWithMatchinbBiData.Target_OpenCitations==\"high\"][\"year\"], num_bins, alpha=0.5, color=\"red\")\n",
    "plt.legend(['Low citations', 'High citations']) \n",
    "ax.set_xlabel('Year of publication')\n",
    "ax.set_ylabel('Citations - low')\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"results/HistBeforeAgeNorm.png\")\n",
    "plt.show()\n",
    "docSubsetWithMatchinbBiData=docSubsetWithMatchinbBiData.drop(columns=['Target_OpenCitations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author names matrix\n",
    "\n",
    "- About author names is better to think like about text - because we want to look for impact of individual authors and also combination of authors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    cvec_authors = CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range=(1,50), min_df=MIN_DF, lowercase = True,strip_accents = \"ascii\", binary= True,stop_words='english')\n",
    "    matrix_authors = cvec_authors.fit_transform(documents['authors'])\n",
    "    tokens = cvec_authors.get_feature_names()\n",
    "    matrix_authors_pd=pd.DataFrame(data=matrix_authors.toarray(), index=documents.index,columns=tokens)\n",
    "\n",
    "    vect_authors_tfidf = TfidfVectorizer( tokenizer = None, ngram_range=(1,50), min_df=MIN_DF, lowercase = True,strip_accents = \"ascii\",stop_words='english')\n",
    "    matrix_authors_tfidf= vect_authors_tfidf.fit_transform(documents['authors'])\n",
    "    tokens_authors_tfidf = vect_authors_tfidf.get_feature_names()\n",
    "    matrix_authors_tfidf_pd=pd.DataFrame(data=matrix_authors_tfidf.toarray(), index=documents.index,columns=tokens_authors_tfidf)\n",
    "\n",
    "    matrix_authors_pd.shape,matrix_authors_tfidf_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Bibliomatric features matrix for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    bibliomet_matrix_randomforest = bibliometric_features.copy()\n",
    "    cols = [\"impact_0_impact\",\"impact_1_impact\",\"ais_0_ais\",\"ais_1_ais\",\"FORD_0_ford\"] \n",
    "    for col in cols:\n",
    "        bibliomet_matrix_randomforest[col] = bibliomet_matrix_randomforest[col].astype(float)\n",
    "    \n",
    "    lb_make = LabelEncoder()\n",
    "    cols = [\"journal\",\"license\",\"WoSkategorie_0_obor\",\"WoSkategorie_0_aisQ\",\"WoSkategorie_0_impactQ\",\"FORD_0_ford\",\"FORD_0_aisQ\",\"FORD_0_impactQ\"]\n",
    "    for col in cols:\n",
    "        bibliomet_matrix_randomforest[col] = lb_make.fit_transform(bibliomet_matrix_randomforest[col])\n",
    "\n",
    "    bibliomet_matrix_randomforest =bibliomet_matrix_randomforest.merge(matrix_authors_pd, how='inner', left_index=True, right_index=True)\n",
    "    bibliomet_matrix_tfidf =bibliomet_matrix_randomforest.merge(matrix_authors_tfidf_pd, how='inner', left_index=True, right_index=True)\n",
    "    bibliomet_matrix_randomforest.shape, bibliomet_matrix_tfidf.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Bibliomatric features matrix for Rulemining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    bibliomet_matrix_rulemining = bibliometric_features.copy()\n",
    "\n",
    "    del bibliomet_matrix_rulemining[\"impact_1_impact\"]\n",
    "    del bibliomet_matrix_rulemining[\"impact_0_impact\"]\n",
    "    del bibliomet_matrix_rulemining[\"ais_0_ais\"]\n",
    "    del bibliomet_matrix_rulemining[\"ais_1_ais\"]\n",
    "    del bibliomet_matrix_rulemining[\"author_count\"]\n",
    "\n",
    "    bibliomet_matrix_rulemining = bibliomet_matrix_rulemining.replace(\"\\xe9\", \"_\") \n",
    "    bibliomet_matrix_rulemining[\"journal\"] = bibliomet_matrix_rulemining[\"journal\"].replace(\" \", \"_\") \n",
    "    bibliomet_matrix_rulemining = pd.get_dummies(bibliomet_matrix_rulemining.astype(str))\n",
    "    bibliomet_matrix_rulemining =bibliomet_matrix_rulemining.merge(matrix_authors_pd, how='inner', left_index=True, right_index=True)\n",
    "    bibliomet_matrix_rulemining = bibliomet_matrix_rulemining.astype(np.int64)\n",
    "\n",
    "    u = bibliomet_matrix_rulemining.select_dtypes(object)\n",
    "    bibliomet_matrix_rulemining[u.columns] = u.apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    bibliomet_matrix_rulemining.columns = [\"\".join(l) for l in bibliomet_matrix_rulemining.columns.str.replace(\"\\s\", \"_\").str.findall(\"[\\w\\d]+\")]\n",
    "    bibliomet_matrix_rulemining.columns = bibliomet_matrix_rulemining.columns.str.replace('\\s+', '_').str.replace('\\W+', '').str.replace('\\xed', '_').str.replace('\\xf1', '').str.replace('\\xfc', '_').str.replace('\\xe9', '_').str.replace( '\\xe7', '')\n",
    "\n",
    "    bibliomet_matrix_rulemining.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce number of rows (to have same number of documents everywhere - like in bibliometric features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    matrix_scispacy_pd = matrix_scispacy_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)\n",
    "    matrix_PubTator_conceptnet_pd = matrix_PubTator_conceptnet_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)\n",
    "    matrix_scispacy_conceptnet_pd= matrix_scispacy_conceptnet_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)\n",
    "    matrix_PubTator_pd=matrix_PubTator_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)\n",
    "    matrix_authors_pd=matrix_authors_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)\n",
    "    matrix_bow_pd=matrix_bow_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)\n",
    "    matrix_tfidf_pd=matrix_tfidf_pd.merge(new_ids, how=\"inner\",left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    bow_plus_bibliometric_features = bibliomet_matrix_randomforest.merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "    tfidf_plus_bibliometric_features = bibliomet_matrix_tfidf.merge(matrix_tfidf_pd, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_scispacy_pd = matrix_scispacy_pd.merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_PubTator_conceptnet_pd = matrix_PubTator_conceptnet_pd.merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_PubTator_pd = matrix_PubTator_pd.merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_scispacy_conceptnet_pd = matrix_scispacy_conceptnet_pd.merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_PubtConc_plus_BibFeat_pd = bow_plus_PubTator_conceptnet_pd.merge(bibliomet_matrix_randomforest, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_bibliometric_features_rule = bibliomet_matrix_rulemining.merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "    bow_plus_bibliometric_features_rule = bow_plus_bibliometric_features_rule.loc[:, bow_plus_bibliometric_features_rule.columns != 'Target']\n",
    "    bow_plus_PubtConc_plus_BibFeat_pd_rule = bibliomet_matrix_rulemining.merge(bow_plus_PubTator_conceptnet_pd , how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal SIZE of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SEARCHING_OPTIMAL_MATRIX_SIZE:\n",
    "    dfs = []\n",
    "    names = []\n",
    "    results = []\n",
    "\n",
    "    min_df_matrix = [ \n",
    "             ('min_df_1', 1),('min_df_4', 4),('min_df_8',8),('min_df_12',12),('min_df_16',16),('min_df_20',20),('min_df_24',24), ('min_df_28',28),('min_df_32',32),('min_df_36',36),     ('min_df_40',40),\n",
    "        ('min_df_44',44),('min_df_48',48), ('min_df_52',52),('min_df_90',90),('min_df_120',120),('min_df_150',150),('min_df_200',200),('min_df_250',250) ]\n",
    "      \n",
    "    for name, min_df_value in min_df_matrix:    \n",
    "        cvec = CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range=(1,3), lowercase = True,strip_accents = \"ascii\", binary= True,stop_words='english',min_df=min_value)    \n",
    "\n",
    "        matrix_bow = cvec.fit_transform(documents['abstract'])\n",
    "        tokens = cvec.get_feature_names()\n",
    "        matrix_bow_pd=pd.DataFrame(data=matrix_bow.toarray(), index=documents.index,columns=tokens)\n",
    "        rf = RandomForestClassifier()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(matrix_bow_pd, documents.Target, test_size=0.3, random_state=1)\n",
    "        kfold = model_selection.KFold(n_splits=3, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(rf, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "        rf.fit(X_train, y_train)        \n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        this_df['number_of_col_in_mtrx'] = matrix_bow_pd.shape[1]\n",
    "        dfs.append(this_df)\n",
    "\n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    final.to_csv(\"results/TUNING_MIN_DF_new.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT TOKENIZATION - FOR RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    documents_bert_1  = documents\n",
    "    documents_bert_1['abstract'] = documents_bert_1['abstract'].str[:512]\n",
    "    tokenized = documents_bert_1['abstract'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    padded = torch.tensor(padded).to(torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model (BERT embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    labels = documents['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest with BERT - classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GRID_FOR_BERT:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = { 'bootstrap': [True],'max_depth': [10,150, 500,1000],'max_features': [30,500,3000], 'min_samples_leaf': [1,10,100], 'min_samples_split': [2,10,100],  'n_estimators': [10, 100]} \n",
    "    rf = RandomForestClassifier()  \n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,  cv = 3, n_jobs = -1, verbose = 2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    rf = RandomForestClassifier(bootstrap= True, max_depth= 500, max_features= 30, min_samples_leaf= 10, min_samples_split= 2, n_estimators= 100)\n",
    "    rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    clsf_report = pd.DataFrame(classification_report(y_true = y_test, y_pred = y_pred, output_dict=True)).transpose()\n",
    "    if DATASET_VERSION ==1: \n",
    "        clsf_report.to_csv('results/randomforest/results_bert_randomforest_datasetv1.csv', index= True)\n",
    "    if DATASET_VERSION ==2: \n",
    "        clsf_report.to_csv('results/randomforest/results_bert_randomforest_datasetv2.csv', index= True)\n",
    "    clsf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest with BERT - regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    labels = documents['NormCitations']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    rf = RandomForestRegressor() \n",
    "    rf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    y_pred = rf.predict(X_test)\n",
    "    MAE = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    y_test_bin = [0 if i <=2.0 else 1 for i in y_test]\n",
    "    y_pred_bin = [0 if i <=2.0 else 1 for i in y_pred]\n",
    "    clsf_report = pd.DataFrame(classification_report(y_true = y_test_bin, y_pred = y_pred_bin, output_dict=True)).transpose()\n",
    "    clsf_report['MAE'] = MAE\n",
    "    clsf_report['MSE'] = MSE\n",
    "    print(clsf_report)\n",
    "    if DATASET_VERSION ==1: \n",
    "        clsf_report.to_csv('results/randomforest/results_bert_randomforest_datasetv1_regression.csv', index= True)\n",
    "    if DATASET_VERSION ==2: \n",
    "        clsf_report.to_csv('results/randomforest/results_bert_randomforest_datasetv2_regression.csv', index= True)\n",
    "    clsf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer for neural network\n",
    "\n",
    "- previous version of model: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1 (now is available https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get module 'bert' has no attribute 'bert_tokenization\n",
    "# pip install bert-tensorflow \n",
    "# pip install bert-for-tf2\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "#watch out we override tokenizer that has been created before with a BERT tokenizer\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_bert_1  = documents\n",
    "documents_bert_1['abstract'] = documents_bert_1['abstract'].str[:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_texts = []\n",
    "sentences = list(documents_bert_1['abstract'])\n",
    "for sen in sentences:\n",
    "    bert_texts.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))\n",
    "tokenized_texts = [tokenize_texts(text) for text in bert_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(classification = True):\n",
    "    \n",
    "    if classification == True:\n",
    "        y = documents_bert_1['Target']\n",
    "        y = np.array(list(map(lambda x: 1 if x==\"high\" else 0, y)))\n",
    "\n",
    "    if classification ==False:\n",
    "        y = documents_bert_1['NormCitations']\n",
    "        y = np.array(list(y))\n",
    "        \n",
    "    add_id=documents_bert_1.reset_index()['doi']\n",
    "    add_id = np.array(list(add_id))\n",
    "\n",
    "    reviews_with_len = [[review, y[i], len(review),  add_id[i]]\n",
    "                     for i, review in enumerate(tokenized_texts)]\n",
    "    for_max_size = [len(review)\n",
    "                     for i, review in enumerate(tokenized_texts)]\n",
    "\n",
    "    max_size=max(for_max_size)\n",
    "\n",
    "    reviews_with_len = [[[review[cnt] if cnt < len(review) else 0 for cnt in range(max_size)], y[i], len(review), add_id[i]]\n",
    "    for i, review in enumerate(tokenized_texts)]\n",
    "\n",
    "    random.shuffle(reviews_with_len)\n",
    "\n",
    "    sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\n",
    "    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "\n",
    "    next(iter(batched_dataset))\n",
    "    \n",
    "    TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "    TEST_BATCHES = TOTAL_BATCHES // 30\n",
    "    test_data = batched_dataset.take(TEST_BATCHES)\n",
    "    train_data = batched_dataset.skip(TEST_BATCHES)\n",
    "\n",
    "    return test_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 regression,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters, kernel_size=2,padding=\"valid\", activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters, kernel_size=3, padding=\"valid\",activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,kernel_size=4,padding=\"valid\",activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        if not regression:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,activation=\"softmax\")\n",
    "        if regression:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,activation=\"relu\") \n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(regression = True):\n",
    "    param1 = {'EMB_DIM' : 200, \n",
    "          'CNN_FILTERS' : 100}\n",
    "    param2 = {'EMB_DIM' : 1400, \n",
    "          'CNN_FILTERS' : 130}\n",
    "    param3 = {'EMB_DIM' : 500, \n",
    "          'CNN_FILTERS' : 200}\n",
    "    param4 = {'EMB_DIM' : 1300, \n",
    "          'CNN_FILTERS' : 50}\n",
    "\n",
    "    if regression:\n",
    "        OUTPUT_CLASSES = 1\n",
    "    if not regression:\n",
    "        OUTPUT_CLASSES = 2\n",
    "\n",
    "    params_mat = [  \n",
    "              ('param1', param1),\n",
    "              ('param2',param2),\n",
    "              ('param3',param3),\n",
    "              ('param4',param4)\n",
    "             ]    \n",
    "\n",
    "    for names,params in params_mat:\n",
    "        text_model = TEXT_MODEL(regression,vocabulary_size=len(tokenizer.vocab),\n",
    "                        embedding_dimensions=params.get('EMB_DIM'),\n",
    "                        cnn_filters=params.get('CNN_FILTERS'),\n",
    "                        dnn_units=256,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=0.2)\n",
    "\n",
    "        if not regression:\n",
    "            text_model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"sparse_categorical_accuracy\"])\n",
    "            test_data, train_data = run_preprocessing(classification = True)\n",
    "        if regression:\n",
    "            text_model.compile(loss=\"mean_squared_error\",optimizer=\"adam\",metrics=[\"mean_squared_error\"])\n",
    "            test_data, train_data = run_preprocessing(classification = False)\n",
    "            \n",
    "        traning = text_model.fit(train_data, epochs=5)\n",
    "        results = text_model.evaluate(test_data)\n",
    "        print(\"results_test_data\")\n",
    "        print(results)\n",
    "    \n",
    "        y_test_bert = np.concatenate([y for x, y in test_data], axis=0)\n",
    "        y_prob = text_model.predict(test_data) \n",
    "        predicted = np.argmax(y_prob, axis=1)\n",
    "\n",
    "        if not regression:\n",
    "            results_pd = pd.DataFrame(classification_report(y_test_bert, predicted, output_dict=True)).transpose()\n",
    "            print(results_pd)\n",
    "            if DATASET_VERSION ==1:\n",
    "                results_pd.to_csv('results/neural_network/results_bow_bert_neural_network_classification_datasetv1_'+str(names)+'.csv', index= True)\n",
    "            if DATASET_VERSION ==2:\n",
    "                results_pd.to_csv('results/neural_network/results_bow_bert_neural_network_classification_datasetv2_'+str(names)+'.csv', index= True)\n",
    "        \n",
    "        if regression: \n",
    "            MAE = mean_absolute_error(y_test_bert, y_prob)\n",
    "            MSE = mean_squared_error(y_test_bert, y_prob)\n",
    "            list_metrics = [MAE, MSE]\n",
    "            keys =[\"MAE\", \"MSE\"]\n",
    "            dictionary = dict(zip(keys, list_metrics))\n",
    "            results_pd = pd.DataFrame.from_dict(dictionary,orient ='index').T \n",
    "            results_pd['model'] = \"BERT\"\n",
    "            results_pd['number_of_col_in_mtrx'] = X_train.shape[1]\n",
    "            if DATASET_VERSION ==2:\n",
    "                y_test_bin = [0 if i <=2.0 else 1 for i in y_test_bert]\n",
    "                y_pred_bin = [0 if i <=2.0 else 1 for i in y_prob] \n",
    "            if DATASET_VERSION ==1:\n",
    "                y_test_bin = [0 if i <=2.0 else 1 for i in y_test_bert]\n",
    "                y_pred_bin = [0 if i <=2.0 else 1 for i in y_prob] \n",
    "            clsf_report = pd.DataFrame(classification_report(y_true = y_test_bin, y_pred = y_pred_bin, output_dict=True)).transpose()\n",
    "            accuracy = clsf_report.iat[2,0]\n",
    "            results_pd['accuracy'] = accuracy\n",
    "            print(results_pd)\n",
    "            if DATASET_VERSION ==1:\n",
    "                results_pd.to_csv('results/neural_network/results_bow_bert_neural_network_regresion_datasetv1'+str(names)+'.csv', index= True)\n",
    "            if DATASET_VERSION ==2:\n",
    "                results_pd.to_csv('results/neural_network/results_bow_bert_neural_network_regresion_datasetv2'+str(names)+'.csv', index= True)\n",
    "          \n",
    "    return print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nn(regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nn(regression=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    matrices = [  ('Scispacy', matrix_scispacy_pd),\n",
    "              ('PubTator_Conceptnet',matrix_PubTator_conceptnet_pd),\n",
    "              ('Scispacy_Conceptnet',matrix_scispacy_conceptnet_pd),\n",
    "              ('PubTator',matrix_PubTator_pd),\n",
    "              ('AuthorsNames',matrix_authors_pd),\n",
    "              ('TF-IDF', matrix_tfidf_pd),\n",
    "              ('Bow',matrix_bow_pd),\n",
    "            # Connected matrices\n",
    "              ('Bow_Scispacy',bow_plus_scispacy_pd), \n",
    "              ('Bow_Scispacy_Conceptnet',bow_plus_scispacy_conceptnet_pd), \n",
    "              ('Bow_PubTator', bow_plus_PubTator_pd),\n",
    "              ('Bow_PubTator_Conceptnet', bow_plus_PubTator_conceptnet_pd),\n",
    "            #Random forest matrices\n",
    "              ('TF-IDF_BibliometricFeatures',tfidf_plus_bibliometric_features),\n",
    "            # Rule mining matrices\n",
    "              ('BibliometricFeatures',bibliomet_matrix_rulemining),\n",
    "              ('Bow_BibliometricFeatures', bow_plus_bibliometric_features_rule),\n",
    "              ('Bow_Pubtator_Conceptnet_BibliometricFeatures', bow_plus_PubtConc_plus_BibFeat_pd_rule)\n",
    "                ]  \n",
    "    \n",
    "if DATASET_VERSION == 2:\n",
    "    matrices = [\n",
    "              ('TF-IDF', matrix_tfidf_pd),\n",
    "              ('Bow',matrix_bow_pd)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GRID_RANDOM_FOREST:\n",
    "    \n",
    "    for name, matrix in matrices: \n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(matrix, target_bib_features.Target, test_size=0.3, random_state=1)\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [10,150, 500,1000],\n",
    "        'max_features': [30,500,3000],\n",
    "        'min_samples_leaf': [1,10,100],\n",
    "        'min_samples_split': [2,10,100],\n",
    "        'n_estimators': [10, 100]\n",
    "        }\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "\n",
    "        grid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid,scoring='accuracy')\n",
    "        grid_search_rf.fit(X_train, y_train)\n",
    "        print(name)\n",
    "        print(grid_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest(CLASSIFIER:bool,USE_FI_FOR_REDUCTION:bool, USE_GRID:bool, matrices_rf:list,matrix_scispacy_pd=None,\n",
    "                      matrix_PubTator_conceptnet_pd=None,matrix_scispacy_conceptnet_pd=None,matrix_PubTator_pd=None,matrix_authors_pd=None,\n",
    "                      matrix_tfidf_pd=None,matrix_bow_pd=None,bow_plus_scispacy_pd=None,bow_plus_scispacy_conceptnet_pd=None,\n",
    "                      bow_plus_PubTator_pd=None,bow_plus_PubTator_conceptnet_pd=None,tfidf_plus_bibliometric_features=None,\n",
    "                      bibliomet_matrix_randomforest=None,bow_plus_bibliometric_features=None,bow_plus_PubtConc_plus_BibFeat_pd=None,\n",
    "                      bibliomet_matrix_rulemining=None,bow_plus_bibliometric_features_rule=None,bow_plus_PubtConc_plus_BibFeat_pd_rule=None):\n",
    "    dfs = []\n",
    "    results = []\n",
    "    names = []\n",
    "    target_names = ['low', 'high']  \n",
    "    \n",
    "    for name, matrix in matrices_rf:\n",
    "        \n",
    "        print(name)\n",
    "        print(matrix.shape)\n",
    "        \n",
    "        if USE_GRID == False:\n",
    "            rf=RandomForestClassifier()  \n",
    "        if USE_GRID == True:  \n",
    "            \n",
    "# # #   Aplying tuning parameters  \n",
    "            if CLASSIFIER == True:\n",
    "                if name==\"Scispacy\":\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 10, max_features= 30, min_samples_leaf= 10, min_samples_split= 100, n_estimators= 1000) \n",
    "                if name=='PubTator_Conceptnet':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 150, max_features= 30, min_samples_leaf= 1, min_samples_split= 100, n_estimators= 10) \n",
    "                if name=='Scispacy_Conceptnet':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 10, max_features= 30, min_samples_leaf= 10, min_samples_split= 100, n_estimators= 100)\n",
    "                if name==\"PubTator\":\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 150, max_features= 30, min_samples_leaf= 1, min_samples_split=100, n_estimators= 100)\n",
    "                if name==\"AuthorsNames\":\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 1000, max_features= 30, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 100)\n",
    "                if name==\"Bow\":\n",
    "                    rf=RandomForestClassifier(bootstrap=True, max_depth= 150, max_features= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_Scispacy':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth=150, max_features= 30, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 100)\n",
    "                if name=='Bow_Scispacy_Conceptnet':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth=500, max_features= 500, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_PubTator':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth=1000, max_features= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_PubTator_Conceptnet':\n",
    "                    rf=RandomForestClassifier(bootstrap=True, max_depth= 150, max_features= 30, min_samples_leaf=1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='TF-IDF':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 1000, max_features= 30, min_samples_leaf= 1, min_samples_split= 100, n_estimators= 100)\n",
    "                if name=='TF-IDF_BibliometricFeatures':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 150, max_features= 30, min_samples_leaf=1, min_samples_split= 100, n_estimators= 100)\n",
    "                if name=='BibliometricFeatures':\n",
    "                    rf=RandomForestClassifier(bootstrap=True, max_depth= 10, max_features= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_BibliometricFeatures':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 500, max_features=30, min_samples_leaf=1, min_samples_split= 100, n_estimators= 100)\n",
    "                if name=='Bow_Pubtator_Conceptnet_BibliometricFeatures':\n",
    "                    rf=RandomForestClassifier(bootstrap= True, max_depth= 500, max_features=500, min_samples_leaf= 1, min_samples_split=2, n_estimators= 100)\n",
    "        \n",
    "            if CLASSIFIER == False:\n",
    "                if name==\"Scispacy\":\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 10, max_features= 30, min_samples_leaf= 10, min_samples_split= 100, n_estimators= 1000) \n",
    "                if name=='PubTator_Conceptnet':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 150, max_features= 30, min_samples_leaf= 1, min_samples_split= 100, n_estimators= 10) \n",
    "                if name=='Scispacy_Conceptnet':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 10, max_features= 30, min_samples_leaf= 10, min_samples_split= 100, n_estimators= 100)\n",
    "                if name==\"PubTator\":\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 150, max_features= 30, min_samples_leaf= 1, min_samples_split=100, n_estimators= 100)\n",
    "                if name==\"AuthorsNames\":\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 1000, max_features= 30, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 100)\n",
    "                if name==\"Bow\":\n",
    "                    rf=RandomForestRegressor(bootstrap=True, max_depth= 150, max_features= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_Scispacy':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth=150, max_features= 30, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 100)\n",
    "                if name=='Bow_Scispacy_Conceptnet':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth=500, max_features= 500, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_PubTator':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth=1000, max_features= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_PubTator_Conceptnet':\n",
    "                    rf=RandomForestRegressor(bootstrap=True, max_depth= 150, max_features= 30, min_samples_leaf=1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='TF-IDF':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 1000, max_features= 30, min_samples_leaf= 1, min_samples_split= 100, n_estimators= 100)\n",
    "                if name=='TF-IDF_BibliometricFeatures':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 150, max_features= 30, min_samples_leaf=1, min_samples_split= 100, n_estimators= 100)\n",
    "                if name=='BibliometricFeatures':\n",
    "                    rf=RandomForestRegressor(bootstrap=True, max_depth= 10, max_features= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "                if name=='Bow_BibliometricFeatures':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 500, max_features=30, min_samples_leaf=1, min_samples_split= 100, n_estimators= 100)\n",
    "                if name=='Bow_Pubtator_Conceptnet_BibliometricFeatures':\n",
    "                    rf=RandomForestRegressor(bootstrap= True, max_depth= 500, max_features=500, min_samples_leaf= 1, min_samples_split=2, n_estimators= 100)\n",
    "\n",
    "            \n",
    "        if not USE_FI_FOR_REDUCTION:\n",
    "            # this model is runing because we need to get feature importance first - only for FI purpose ! Is not evaluated, no saved results - not needed.\n",
    "            if CLASSIFIER == False:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(matrix, documents.NormCitations, test_size=0.3, random_state=1) \n",
    "            \n",
    "            if CLASSIFIER == True:\n",
    "                if DATASET_VERSION == 1:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(matrix, documents.Target, test_size=0.3, random_state=1)\n",
    "                if DATASET_VERSION == 2:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(matrix, documents.Target, test_size=0.3, random_state=1) \n",
    "          \n",
    "                \n",
    "            rf.fit(X_train, y_train)  \n",
    "            importance = pd.Series(rf.feature_importances_, index=X_train.columns).nlargest(10000)\n",
    "            importance.to_csv(\"cache/FeatureImportance/importance_RF_\" +name +'.csv')  \n",
    "            final = \"Done\"\n",
    "        \n",
    "        if USE_FI_FOR_REDUCTION:   \n",
    "            importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_\"+name+\".csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "            list_of_features = importance['Unnamed: 0'].to_list()\n",
    "            selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "            \n",
    "            if name == \"Scispacy\":\n",
    "                matrix = matrix_scispacy_pd[selected_features]   #  reduce the matrix     \n",
    "            if name == \"PubTator_Conceptnet\":\n",
    "                matrix= matrix_PubTator_conceptnet_pd[selected_features]   #  reduce the matrix \n",
    "            if name == \"Scispacy_Conceptnet\":\n",
    "                matrix = matrix_scispacy_conceptnet_pd[selected_features]#  reduce the matrix \n",
    "            if name == \"PubTator\":\n",
    "                matrix = matrix_PubTator_pd[selected_features] #  reduce the matrix \n",
    "            if name ==\"AuthorsNames\":\n",
    "                matrix = matrix_authors_pd\n",
    "            if name ==\"Bow\":\n",
    "                matrix = matrix_bow_pd\n",
    "            if name ==\"TF-IDF\":\n",
    "                matrix = matrix_tfidf_pd\n",
    "            if name ==\"Bow_Scispacy\":\n",
    "                importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_Scispacy.csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "                list_of_features = importance['Unnamed: 0'].to_list()\n",
    "                selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "                matrix = matrix_scispacy_pd[selected_features].merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "            if name ==\"Bow_Scispacy_Conceptnet\":\n",
    "                importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_Scispacy_Conceptnet.csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "                list_of_features = importance['Unnamed: 0'].to_list()\n",
    "                selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "                matrix = matrix_scispacy_conceptnet_pd[selected_features].merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "            if name ==\"Bow_PubTator\":\n",
    "                importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_PubTator.csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "                list_of_features = importance['Unnamed: 0'].to_list()\n",
    "                selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "                matrix = matrix_PubTator_pd[selected_features].merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)       \n",
    "            if name ==\"Bow_PubTator_Conceptnet\":\n",
    "                importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_PubTator_Conceptnet.csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "                list_of_features = importance['Unnamed: 0'].to_list()\n",
    "                selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "                matrix = matrix_PubTator_conceptnet_pd[selected_features].merge(matrix_bow_pd, how='inner', left_index=True, right_index=True)\n",
    "            if name ==\"TF-IDF_BibliometricFeatures\":\n",
    "                matrix = tfidf_plus_bibliometric_features\n",
    "            if name ==\"BibliometricFeatures_rf\":\n",
    "                matrix = bibliomet_matrix_randomforest\n",
    "            if name ==\"Bow_BibliometricFeatures_rf\":\n",
    "                matrix = bow_plus_bibliometric_features\n",
    "            if name ==\"BibliometricFeatures_rule\":\n",
    "                matrix = bibliomet_matrix_rulemining\n",
    "            if name ==\"Bow_BibliometricFeatures_rule\":\n",
    "                matrix = bow_plus_bibliometric_features_rule    \n",
    "            if name==\"Bow_Pubtator_Conceptnet_BibliometricFeatures_rf\":\n",
    "                importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_PubTator_Conceptnet.csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "                list_of_features = importance['Unnamed: 0'].to_list()\n",
    "                selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "                matrix = matrix_PubTator_conceptnet_pd[selected_features].merge(bow_plus_bibliometric_features, how='inner', left_index=True, right_index=True)\n",
    "            if name==\"Bow_Pubtator_Conceptnet_BibliometricFeatures_rule\":\n",
    "                importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_PubTator_Conceptnet.csv\", index_col=False, header=0).sort_values(by=[\"0\"],ascending=False)\n",
    "                list_of_features = importance['Unnamed: 0'].to_list()\n",
    "                selected_features = itertools.islice(list_of_features, MAX_NO_OF_FEATURES_SCISPACY_AND_PUBT) # grab the first x elements\n",
    "                matrix = matrix_PubTator_conceptnet_pd[selected_features].merge(bow_plus_bibliometric_features_rule, how='inner', left_index=True, right_index=True)\n",
    "                \n",
    "            if CLASSIFIER == True:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(matrix, documents.Target, test_size=0.3, random_state=1)\n",
    "                print(X_train.shape)\n",
    "                X_train.to_csv(\"results/matrices/X_train_\"+name+\"_rule.csv\") # save for rule mining\n",
    "                y_train.to_csv(\"results/matrices/y_train_\"+name+\"_rule.csv\")\n",
    "                X_test.to_csv(\"results/matrices/X_test_\"+name+\"_rule.csv\")\n",
    "                y_test.to_csv(\"results/matrices/y_test_\"+name+\"_rule.csv\")  \n",
    "            \n",
    "            if CLASSIFIER == False:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(matrix, documents.NormCitations, test_size=0.3, random_state=1)\n",
    "            \n",
    "            \n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            if CLASSIFIER == True:\n",
    "                ## Evaluation of model\n",
    "                y_pred = rf.predict(X_test)     \n",
    "                clsf_report = pd.DataFrame(classification_report(y_true = y_test, y_pred = y_pred, output_dict=True)).transpose()\n",
    "                accuracy = clsf_report.iat[2,0]\n",
    "                precision = clsf_report.iat[4,0]\n",
    "                recall = clsf_report.iat[4,1]\n",
    "                f1_score = clsf_report.iat[4,2]\n",
    "                list_metrics = [accuracy, precision, recall, f1_score]\n",
    "                keys =[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "                dictionary = dict(zip(keys, list_metrics))\n",
    "                results_pd = pd.DataFrame.from_dict(dictionary,orient ='index').T \n",
    "                results_pd['model'] = name\n",
    "                results_pd['number_of_col_in_mtrx'] = X_train.shape[1]\n",
    "                dfs.append(results_pd)\n",
    "                print(classification_report(y_test, y_pred, target_names=target_names))   \n",
    "            \n",
    "            if CLASSIFIER == False:\n",
    "                y_pred = rf.predict(X_test)\n",
    "                MAE = mean_absolute_error(y_test, y_pred)\n",
    "                MSE = mean_squared_error(y_test, y_pred)\n",
    "                list_metrics = [MAE, MSE]\n",
    "                keys =[\"MAE\", \"MSE\"]\n",
    "                dictionary = dict(zip(keys, list_metrics))\n",
    "                results_pd = pd.DataFrame.from_dict(dictionary,orient ='index').T \n",
    "                results_pd['model'] = name\n",
    "                results_pd['number_of_col_in_mtrx'] = X_train.shape[1]\n",
    "                y_test_bin = [0 if i <=2.0 else 1 for i in y_test]\n",
    "                y_pred_bin = [0 if i <=2.0 else 1 for i in y_pred]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(matrix, documents.Target, test_size=0.3, random_state=1)\n",
    "                clsf_report = pd.DataFrame(classification_report(y_true = y_test_bin, y_pred = y_pred_bin, output_dict=True)).transpose()\n",
    "                accuracy = clsf_report.iat[2,0]\n",
    "                results_pd['accuracy'] = accuracy\n",
    "                \n",
    "                dfs.append(results_pd)\n",
    "        \n",
    "    if USE_FI_FOR_REDUCTION:\n",
    "        final = pd.concat(dfs, ignore_index=True)\n",
    "        if DATASET_VERSION ==1:\n",
    "            if CLASSIFIER ==True:\n",
    "                final.to_csv(\"results/randomforest/results_RF_datasetv1.csv\") \n",
    "            if CLASSIFIER ==False:\n",
    "                final.to_csv(\"results/randomforest/results_RF_datasetv1_regression.csv\") \n",
    "        if DATASET_VERSION ==2:\n",
    "            if CLASSIFIER ==True:\n",
    "                final.to_csv(\"results/randomforest/results_RF_datasetv2.csv\")\n",
    "            if CLASSIFIER ==False:\n",
    "                final.to_csv(\"results/randomforest/results_RF_datasetv2_regression.csv\") \n",
    "           \n",
    "    return print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running to get FI for all matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_random_forest(CLASSIFIER = True,USE_FI_FOR_REDUCTION=False, USE_GRID=True, matrices_rf=matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running to get results for reduced matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    run_random_forest(CLASSIFIER = True,USE_FI_FOR_REDUCTION=True, USE_GRID=True, matrices_rf=matrices,\n",
    "                  matrix_scispacy_pd= matrix_scispacy_pd,\n",
    "                  matrix_PubTator_conceptnet_pd=matrix_PubTator_conceptnet_pd,\n",
    "                  matrix_scispacy_conceptnet_pd=matrix_scispacy_conceptnet_pd,\n",
    "                  matrix_PubTator_pd=matrix_PubTator_pd,\n",
    "                  matrix_authors_pd=matrix_authors_pd,\n",
    "                  matrix_tfidf_pd=matrix_tfidf_pd,\n",
    "                  matrix_bow_pd=matrix_bow_pd,\n",
    "                  bow_plus_scispacy_pd=bow_plus_scispacy_pd,\n",
    "                  bow_plus_scispacy_conceptnet_pd=bow_plus_scispacy_conceptnet_pd,\n",
    "                  bow_plus_PubTator_pd=bow_plus_PubTator_pd,\n",
    "                  bow_plus_PubTator_conceptnet_pd=bow_plus_PubTator_conceptnet_pd,\n",
    "                  tfidf_plus_bibliometric_features=tfidf_plus_bibliometric_features,\n",
    "                  bibliomet_matrix_randomforest= bibliomet_matrix_randomforest,\n",
    "                  bow_plus_bibliometric_features=bow_plus_bibliometric_features,\n",
    "                  bow_plus_PubtConc_plus_BibFeat_pd=bow_plus_PubtConc_plus_BibFeat_pd,\n",
    "                  bibliomet_matrix_rulemining=bibliomet_matrix_rulemining,\n",
    "                  bow_plus_bibliometric_features_rule=bow_plus_bibliometric_features_rule,\n",
    "                  bow_plus_PubtConc_plus_BibFeat_pd_rule=bow_plus_PubtConc_plus_BibFeat_pd_rule)\n",
    "    \n",
    "    run_random_forest(CLASSIFIER = False,USE_FI_FOR_REDUCTION=True, USE_GRID=True, matrices_rf=matrices,\n",
    "                  matrix_scispacy_pd= matrix_scispacy_pd,\n",
    "                  matrix_PubTator_conceptnet_pd=matrix_PubTator_conceptnet_pd,\n",
    "                  matrix_scispacy_conceptnet_pd=matrix_scispacy_conceptnet_pd,\n",
    "                  matrix_PubTator_pd=matrix_PubTator_pd,\n",
    "                  matrix_authors_pd=matrix_authors_pd,\n",
    "                  matrix_tfidf_pd=matrix_tfidf_pd,\n",
    "                  matrix_bow_pd=matrix_bow_pd,\n",
    "                  bow_plus_scispacy_pd=bow_plus_scispacy_pd,\n",
    "                  bow_plus_scispacy_conceptnet_pd=bow_plus_scispacy_conceptnet_pd,\n",
    "                  bow_plus_PubTator_pd=bow_plus_PubTator_pd,\n",
    "                  bow_plus_PubTator_conceptnet_pd=bow_plus_PubTator_conceptnet_pd,\n",
    "                  tfidf_plus_bibliometric_features=tfidf_plus_bibliometric_features,\n",
    "                  bibliomet_matrix_randomforest= bibliomet_matrix_randomforest,\n",
    "                  bow_plus_bibliometric_features=bow_plus_bibliometric_features,\n",
    "                  bow_plus_PubtConc_plus_BibFeat_pd=bow_plus_PubtConc_plus_BibFeat_pd,\n",
    "                  bibliomet_matrix_rulemining=bibliomet_matrix_rulemining,\n",
    "                  bow_plus_bibliometric_features_rule=bow_plus_bibliometric_features_rule,\n",
    "                  bow_plus_PubtConc_plus_BibFeat_pd_rule=bow_plus_PubtConc_plus_BibFeat_pd_rule)\n",
    "    \n",
    "if DATASET_VERSION ==2:\n",
    "    run_random_forest(CLASSIFIER = True,USE_FI_FOR_REDUCTION=True, USE_GRID=False,\n",
    "                  matrices_rf = matrices,\n",
    "                  matrix_tfidf_pd=matrix_tfidf_pd,\n",
    "                  matrix_bow_pd=matrix_bow_pd\n",
    "               )\n",
    "    run_random_forest(CLASSIFIER = False,USE_FI_FOR_REDUCTION=True, USE_GRID=False,\n",
    "                  matrices_rf = matrices,\n",
    "                  matrix_tfidf_pd=matrix_tfidf_pd,\n",
    "                  matrix_bow_pd=matrix_bow_pd\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, matrix in matrices: \n",
    "    if DATASET_VERSION ==1:\n",
    "        y = documents.Target\n",
    "    if DATASET_VERSION ==2:\n",
    "        y = documents.Target\n",
    "    rf=RandomForestClassifier()\n",
    "    importance = pd.read_csv(\"cache/FeatureImportance/importance_RF_\"+name+\".csv\", index_col=False, header=0)\n",
    "    importance = importance.sort_values(by=[\"0\"],ascending=False)\n",
    "    list_of_features = importance['Unnamed: 0'].to_list()\n",
    "    selected_features = itertools.islice(list_of_features, 25) \n",
    "    matrix_reduced=matrix[selected_features]   #  reduce the matrix \n",
    "    X = matrix_reduced\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3,random_state=1)  \n",
    "    rf.fit(X_train, Y_train)  \n",
    "    shap_values = shap.TreeExplainer(rf).shap_values(X)\n",
    "    print(name)\n",
    "    f = plt.figure()\n",
    "    shap.summary_plot(shap_values[0], X)\n",
    "    f.savefig(\"results/randomforest/summary_\"+name+\".pdf\", bbox_inches='tight')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one :\n",
    "#TEXT = documents[\"authors\"]\n",
    "TEXT = documents[\"abstract\"]\n",
    "\n",
    "#vectorizer = TfidfVectorizer(min_df= 10, stop_words={'english'},ngram_range=(1,3))\n",
    "vectorizer =  CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range=(1,3),lowercase = True,strip_accents = \"ascii\", binary= True, stop_words='english',min_df=MIN_DF)\n",
    "\n",
    "list_labels = documents[\"Target\"].tolist()\n",
    "list_corpus = TEXT.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.3, random_state=1)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_vectors, y_train)   \n",
    "pred = rf.predict(test_vectors)\n",
    "c = make_pipeline(vectorizer, rf)\n",
    "class_names=list(documents.Target.unique())\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "idx = 55   # 400 , 404 , 24, 28, 135\n",
    "\n",
    "exp = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=100, labels=[0, 1])\n",
    "exp.show_in_notebook(text=y_test[idx], labels=(1,))\n",
    "\n",
    "exp =  explainer.explain_instance(X_test[idx], c.predict_proba, num_features=20, labels=(1,))\n",
    "\n",
    "exp.save_to_file('results/lime/lime4.html')\n",
    "\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RULE MINING MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dataset version 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    matrices_rule = [     \n",
    "              ('Scispacy', matrix_scispacy_pd),\n",
    "              ('PubTator_Conceptnet',matrix_PubTator_conceptnet_pd),\n",
    "              ('Scispacy_Conceptnet',matrix_scispacy_conceptnet_pd),\n",
    "              ('PubTator',matrix_PubTator_pd),\n",
    "              ('AuthorsNames',matrix_authors_pd),  \n",
    "              ('Bow',matrix_bow_pd),\n",
    "              ('Bow_Scispacy',bow_plus_scispacy_pd), \n",
    "              ('Bow_Scispacy_Conceptnet',bow_plus_scispacy_conceptnet_pd), \n",
    "              ('Bow_PubTator',bow_plus_PubTator_pd),\n",
    "              ('Bow_PubTator_Conceptnet',bow_plus_PubTator_conceptnet_pd),\n",
    "              ('BibliometricFeatures',bibliomet_matrix_rulemining),\n",
    "              ('Bow_BibliometricFeatures', bow_plus_bibliometric_features_rule),\n",
    "              ('Bow_Pubtator_Conceptnet_BibliometricFeatures', bow_plus_PubtConc_plus_BibFeat_pd_rule)\n",
    "          ]      \n",
    "    target_names = ['low', 'high']  # there I am not sure what column is what value of target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_list  = []\n",
    "dfs = []\n",
    "results = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    corels=CorelsClassifier(n_iter=50, c=0.0000, max_card=2, min_support=0.3)\n",
    "\n",
    "    for name, matrix in matrices_rule: \n",
    "        print(name)\n",
    "        X_train = pd.read_csv(\"results/matrices/X_train_\"+name+\"_rule.csv\" ,index_col='doi')\n",
    "        y_train = pd.read_csv(\"results/matrices/y_train_\"+name+\"_rule.csv\" ,index_col='doi')\n",
    "        X_test = pd.read_csv(\"results/matrices/X_test_\"+name+\"_rule.csv\",index_col='doi')\n",
    "        y_test = pd.read_csv(\"results/matrices/y_test_\"+name+\"_rule.csv\",index_col='doi')  \n",
    "        y_train = np.array(list(map(lambda x: 1 if x==\"high\" else 0, y_train['Target'])))\n",
    "        y_test =  np.array(list(map(lambda x: 1 if x==\"high\" else 0, y_test['Target'])))\n",
    "    \n",
    "        X_train = X_train.reset_index()\n",
    "        del X_train['doi']\n",
    "        X_train.columns = X_train.columns.map(str)    \n",
    "        features =[]\n",
    "        for col in X_train.columns: \n",
    "                features.append(col)\n",
    "                 \n",
    "        corels=corels.fit(X_train, y_train, features=features, prediction_name=\"high_citation\")       \n",
    "        y_pred = corels.predict(X_test) \n",
    "        antLengths = list(map(lambda r: len(r[\"antecedents\"]), corels.rl().rules))\n",
    "        avgRuleLengthCorels = sum(antLengths) / len(antLengths)\n",
    "        ruleCountCorels = len(corels.rl().rules)\n",
    "   \n",
    "        names.append(name)\n",
    "        clsf_report = pd.DataFrame(classification_report(y_true = y_test, y_pred = y_pred, output_dict=True)).transpose()\n",
    "        print(name)\n",
    "        print(clsf_report)\n",
    "        print(type(antLengths))\n",
    "        print(antLengths)\n",
    "        print(type(avgRuleLengthCorels))\n",
    "        print(type(ruleCountCorels))\n",
    "    \n",
    "        accuracy = clsf_report.iat[2,0]\n",
    "        precision = clsf_report.iat[4,0]\n",
    "        recall = clsf_report.iat[4,1]\n",
    "        f1_score = clsf_report.iat[4,2]\n",
    "        list_metrics = [accuracy, precision, recall, f1_score]\n",
    "        keys =[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "        dictionary = dict(zip(keys, list_metrics))\n",
    "        results_pd = pd.DataFrame.from_dict(dictionary,orient ='index').T \n",
    "        results_pd['model'] = name\n",
    "        results_pd['number_of_col_in_mtrx'] = X_train.shape[1]\n",
    "    \n",
    "        results_pd['antLengths'] = antLengths[0]\n",
    "        results_pd['avgRuleLengthCorels'] = avgRuleLengthCorels\n",
    "        results_pd['ruleCountCorels'] = ruleCountCorels\n",
    "        \n",
    "        rules=corels.rl()\n",
    "        f = open(\"results/rule_lists/results_\"+name+\"_rules_corels.csv\",\"w\")\n",
    "        f.write(str(rules))\n",
    "        f.close()\n",
    "        rules_pd = pd.read_csv(\"results/rule_lists/results_\"+name+\"_rules_corels.csv\")\n",
    "        rules_pd['model'] = name\n",
    "    \n",
    "        rules_list.append(rules_pd)\n",
    "        dfs.append(results_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    final.to_csv(\"results/corels/RESULTS_CORELS.csv\")   \n",
    "    rules_final = pd.concat(rules_list, ignore_index=True)\n",
    "    rules_final.to_csv(\"results/corels/RULES_CORELS.csv\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    \n",
    "    rules_list  = []\n",
    "    dfs = []\n",
    "\n",
    "    for name, matrix in matrices_rule: \n",
    "        X_train = pd.read_csv(\"results/matrices/X_train_\"+name+\"_rule.csv\" ,index_col='doi')\n",
    "        y_train = pd.read_csv(\"results/matrices/y_train_\"+name+\"_rule.csv\" ,index_col='doi')\n",
    "        X_test = pd.read_csv(\"results/matrices/X_test_\"+name+\"_rule.csv\",index_col='doi')\n",
    "        y_test = pd.read_csv(\"results/matrices/y_test_\"+name+\"_rule.csv\",index_col='doi') \n",
    "    \n",
    "        X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_test.shape)\n",
    "        print(y_train.shape)\n",
    "    \n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            X_train_R = ro.conversion.py2rpy(X_train)\n",
    "            X_test_R = ro.conversion.py2rpy(X_test)\n",
    "            y_test_R = ro.conversion.py2rpy(y_test)\n",
    "            y_train_R = ro.conversion.py2rpy(y_train)\n",
    "        \n",
    "            print(type(X_train_R))\n",
    "                    \n",
    "        robjects.globalenv[\"trainFold_X\"] = X_train_R\n",
    "        robjects.globalenv[\"trainFold_Y\"] = y_train_R\n",
    "        robjects.globalenv[\"testFold_X\"] = X_test_R\n",
    "        robjects.globalenv[\"testFold_Y\"] = y_test_R\n",
    "        robjects.globalenv[\"exp_name\"] = name\n",
    "\n",
    "        # 500 for fast learning, 50000 for best results\n",
    "        robjects.globalenv[\"candidate_rule_limit\"] = 50000\n",
    "    \n",
    "        print(name)\n",
    "        \n",
    "        # Execute the CBA pipeline\n",
    "        # read R program with CBA \n",
    "        rscript = open('cordParam.R', 'r').read()\n",
    "        result = robjects.r(rscript)\n",
    "    \n",
    "        pred_perf_df = robjects.globalenv[\"pred_perf_df\"]\n",
    "        print(pred_perf_df)\n",
    "        pred_perf_df.to_csv(\"results/rule_lists/results_cba_50000_\" +name +'.csv')\n",
    "        results_pd = pd.read_csv(\"results/rule_lists/results_cba_50000_\" +name +'.csv')\n",
    "        results_pd['model'] = name\n",
    "        results_pd['number_of_col_in_mtrx'] = X_train.shape[1]\n",
    "        rules_length = robjects.globalenv[\"rules_length\"][0]\n",
    "        avgRuleLengthCBA = robjects.globalenv[\"avgRuleLengthCBA\"][0]\n",
    "        results_pd['rules_length']=rules_length\n",
    "        results_pd['avgRuleLengthCBA']=avgRuleLengthCBA  \n",
    "        print(results_pd)\n",
    "        dfs.append(results_pd) \n",
    "    \n",
    "        confusion_matrix_cba = robjects.globalenv[\"confusion_matrix_df\"] #also on disk\n",
    "        print(confusion_matrix_cba)\n",
    "    \n",
    "        rules_cba = robjects.globalenv[\"rules_df\"]\n",
    "        rules_cba.to_csv(\"results/rule_lists/rules_cba_50000_\" +name +'.csv')\n",
    "        rules_pd = pd.read_csv(\"results/rule_lists/rules_cba_50000_\" +name +'.csv')\n",
    "        rules_pd['model'] = name\n",
    "        rules_list.append(rules_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    final.to_csv(\"results/cba/RESULTS_CBA.csv\")  \n",
    "    rules_final = pd.concat(rules_list, ignore_index=True)\n",
    "    rules_final.to_csv(\"results/cba/RULES_CBA.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION ==1:\n",
    "    rscript = open('cordParamVizualization.R', 'r').read()\n",
    "    result = robjects.r(rscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author names analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to detect number of unique author names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: it is not possible to detect number of unique names, because \n",
    "            many same names are written in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    \n",
    "    authors_series = documents['authors']\n",
    "    authors_series_pd = authors_series.reset_index()\n",
    "    authors_new = []\n",
    "    for row in list(authors_series_pd.doi.values):\n",
    "        authors_series_pd_f = authors_series_pd[authors_series_pd['doi']==row]\n",
    "    \n",
    "        if not authors_series_pd_f.authors.values[0].startswith(\"[\"):\n",
    "            new_authors_value = authors_series_pd_f.authors.values[0]\n",
    "            new_authors_value = new_authors_value.replace(\",\",\"_\").split(\";\")\n",
    "            new_authors_value = [word for word in new_authors_value]\n",
    "             \n",
    "        if authors_series_pd_f.authors.values[0].startswith(\"[\"):\n",
    "            new_authors_value = authors_series_pd_f.authors.values[0]\n",
    "            new_authors_value = ast.literal_eval(new_authors_value)\n",
    "            new_authors_value = [word.replace(',','_') for word in new_authors_value]\n",
    "         \n",
    "        authors_new.append(new_authors_value)\n",
    "  \n",
    "    authors_series_pd[\"authors_new\"] = authors_new\n",
    "\n",
    "    authors_series_pd[\"authors_new\"] = authors_series_pd[\"authors_new\"].astype(str).str.replace(']','').str.replace('[','')\n",
    "    dummies = authors_series_pd[\"authors_new\"].str.get_dummies(',')\n",
    "    print(\"Names\")\n",
    "    print(list(dummies.columns))\n",
    "    print(\"Number of names:\"+str(len(list(dummies.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    cvec_authors = CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range=(1,50), min_df=MIN_DF, lowercase = True,strip_accents = \"ascii\", binary= True,stop_words='english')\n",
    "    matrix_authors = cvec_authors.fit_transform(documents['authors'])\n",
    "    tokens = cvec_authors.get_feature_names()\n",
    "    matrix_authors_pd=pd.DataFrame(data=matrix_authors.toarray(), index=documents.index,columns=tokens)\n",
    "\n",
    "    vect_authors_tfidf = TfidfVectorizer( tokenizer = None, ngram_range=(1,50), min_df=MIN_DF, lowercase = True,strip_accents = \"ascii\",stop_words='english')\n",
    "    matrix_authors_tfidf= vect_authors_tfidf.fit_transform(documents['authors'])\n",
    "    tokens_authors_tfidf = vect_authors_tfidf.get_feature_names()\n",
    "    matrix_authors_tfidf_pd=pd.DataFrame(data=matrix_authors_tfidf.toarray(), index=documents.index,columns=tokens_authors_tfidf)\n",
    "\n",
    "    matrix_authors_pd.shape,matrix_authors_tfidf_pd.shape\n",
    "    matrix_authors_pd.sum().reset_index().sort_values(0,ascending=False).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    authors_target = matrix_authors_pd.join(documents[[\"Target\"]]).groupby(\"Target\").sum().transpose()\n",
    "    authors_target.columns = authors_target.columns.astype(str)\n",
    "    authors_target = authors_target.reset_index()\n",
    "    authors_target[\"Total\"] = authors_target[\"low\"]+authors_target[\"high\"]\n",
    "    authors_target[\"frac\"] = authors_target[\"low\"] / authors_target[\"Total\"]\n",
    "    authors_target.sort_values(\"Total\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:   \n",
    "    jmena = pd.read_csv(\"inputs/author_names_info.csv\",delimiter = \";\")\n",
    "    jmena[\"frac\"] = jmena[\"frac\"].str.replace(\",\",\".\").astype(float) \n",
    "    jmena[\"frac\"] = jmena[\"high\"]/jmena[\"Total\"]\n",
    "    print(jmena.head(50))\n",
    "    print(jmena[  (jmena[\"W_E\"]!=\"Arabia\") ][[\"W_E\"]].value_counts().reset_index().sort_values(\"W_E\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    num_bins = len(list(set(list(jmena[\"frac\"].values))))\n",
    "    ax.hist(jmena[jmena.W_E==\"Europe\"][\"frac\"], alpha=0.5,color=\"red\",bins=20)\n",
    "    ax.hist(jmena[jmena.W_E==\"Asia\"][\"frac\"], alpha=0.5,bins=20)\n",
    "\n",
    "    plt.legend(['European names', 'Asia names'],fontsize=12) \n",
    "    plt.axvline(jmena[jmena.W_E==\"Asia\"][\"frac\"].mean(), color='b', linestyle='dashed')\n",
    "    plt.text(jmena[jmena.W_E==\"Asia\"][\"frac\"].mean(),11,'Mean (Asia): {:.2f}'.format(jmena[jmena.W_E==\"Asia\"][\"frac\"].mean()),fontsize=12)\n",
    "    plt.axvline(jmena[jmena.W_E==\"Asia\"][\"frac\"].median(), color='b', linestyle='dashed')\n",
    "    plt.text(jmena[jmena.W_E==\"Asia\"][\"frac\"].median(),10,'Median (Asia): {:.2f}'.format(jmena[jmena.W_E==\"Asia\"][\"frac\"].median()),fontsize=12)\n",
    "    plt.axvline(jmena[jmena.W_E==\"Europe\"][\"frac\"].mean(), color='r', linestyle='dashed')\n",
    "    plt.text(jmena[jmena.W_E==\"Europe\"][\"frac\"].mean(),6,'Mean (Europe): {:.2f}'.format(jmena[jmena.W_E==\"Europe\"][\"frac\"].mean()),fontsize=12)\n",
    "    plt.text(jmena[jmena.W_E==\"Europe\"][\"frac\"].median(),7,'Median (Europe): {:.2f}'.format(jmena[jmena.W_E==\"Europe\"][\"frac\"].median()),fontsize=12)\n",
    "    plt.axvline(jmena[jmena.W_E==\"Europe\"][\"frac\"].median(), color='r', linestyle='dashed')\n",
    "    ax.set_xlabel('Percentage of highly cited articles for each name',fontsize=13)\n",
    "    ax.set_ylabel('Number of names',fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    x = jmena[jmena.W_E==\"Asia\"][\"frac\"]\n",
    "    y = jmena[jmena.W_E==\"Europe\"][\"frac\"]\n",
    "    U1, p = mannwhitneyu(x, y)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    from scipy.stats import ttest_ind\n",
    "    stat, p = ttest_ind(x, y)\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('Probably the same distribution')\n",
    "    else:\n",
    "        print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    df_1 = matrix_authors_pd.where(matrix_authors_pd.eq(1)).stack().reset_index(level=1)['level_1'].reset_index().join(documents[['OpenCitations']],on=\"doi\",how=\"left\")\n",
    "    df_2 = pd.merge(jmena[['name',\"W_E\"]],df_1, left_on='name', right_on='level_1')\n",
    "    df_2 = df_2[df_2[\"OpenCitations\"]<250]\n",
    "\n",
    "    num_bins = len(list(set(list(df_2[\"OpenCitations\"].values))))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.hist(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"],alpha=0.5,color=\"r\",bins=30)\n",
    "    ax.hist(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"], alpha=0.5,bins=30)\n",
    "    plt.legend(['European names', 'Asia names'],fontsize=12) \n",
    "    plt.axvline(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"].mean(), color='b', linestyle='dashed')\n",
    "    plt.text(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"].mean(),400,'Mean (Asia): {:.2f}'.format(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"].mean()),fontsize=12)\n",
    "    plt.axvline(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"].median(), color='b', linestyle='dashed')\n",
    "    plt.text(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"].median(),500,'Median (Asia): {:.2f}'.format(df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"].median()),fontsize=12)\n",
    "    plt.axvline(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"].mean(), color='r', linestyle='dashed')\n",
    "    plt.text(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"].mean(),600,'Mean (Asia): {:.2f}'.format(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"].mean()),fontsize=12)\n",
    "    plt.text(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"].median(),700,'Median (Asia): {:.2f}'.format(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"].median()),fontsize=12)\n",
    "    plt.axvline(df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"].median(), color='r', linestyle='dashed')\n",
    "    ax.set_xlabel('Number of citations of articles',fontsize=12)\n",
    "    ax.set_ylabel('Number of articles by author continent',fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    x = df_2[df_2.W_E==\"Asia\"][\"OpenCitations\"]\n",
    "    y = df_2[df_2.W_E==\"Europe\"][\"OpenCitations\"]\n",
    "    U1, p = mannwhitneyu(x, y)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_VERSION == 1:\n",
    "    stat, p = ttest_ind(x, y)\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('Probably the same distribution')\n",
    "    else:\n",
    "        print('Probably different distributions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
